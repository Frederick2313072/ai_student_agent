"""
KnowledgeValidator Agent - çŸ¥è¯†éªŒè¯Agent

è´Ÿè´£éªŒè¯ç”¨æˆ·è§£é‡Šçš„äº‹å®å‡†ç¡®æ€§ã€æ£€æŸ¥æ¦‚å¿µå®šä¹‰çš„æ­£ç¡®æ€§ã€è¯†åˆ«å¸¸è§è¯¯è§£ã€‚
è¿™ä¸ªAgentä¸“é—¨å¤„ç†çŸ¥è¯†çš„å‡†ç¡®æ€§å’Œå¯é æ€§é—®é¢˜ã€‚
"""

import json
import time
from typing import List, Dict, Any, Optional
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatZhipuAI
from pydantic import BaseModel, Field

from .agent_protocol import (
    AgentInterface, AgentMetadata, AgentCapability, AgentType, AgentTask, 
    ConversationContext, AgentResponse, TaskType, create_response
)
from feynman.core.config.settings import get_settings


# =============================================================================
# å†…åµŒæç¤ºè¯æ¨¡æ¿
# =============================================================================

KNOWLEDGE_VALIDATOR_SYSTEM_PROMPT = """ä½ æ˜¯è´¹æ›¼å­¦ä¹ ç³»ç»Ÿçš„çŸ¥è¯†éªŒè¯ä¸“å®¶Agentï¼Œè´Ÿè´£éªŒè¯ç”¨æˆ·è§£é‡Šçš„äº‹å®å‡†ç¡®æ€§å’Œæ¦‚å¿µæ­£ç¡®æ€§ã€‚

## ğŸ¯ æ ¸å¿ƒèŒè´£

1. **äº‹å®å‡†ç¡®æ€§éªŒè¯**
   - æ£€æŸ¥å…·ä½“äº‹å®ã€æ•°æ®å’Œç»Ÿè®¡ä¿¡æ¯çš„å‡†ç¡®æ€§
   - éªŒè¯å†å²äº‹ä»¶ã€æ—¥æœŸå’Œäººç‰©ä¿¡æ¯
   - ç¡®è®¤ç§‘å­¦å®šå¾‹ã€å…¬å¼å’ŒåŸç†çš„æ­£ç¡®æ€§

2. **æ¦‚å¿µå®šä¹‰æ£€æŸ¥**
   - éªŒè¯ä¸“ä¸šæœ¯è¯­å’Œæ¦‚å¿µçš„å®šä¹‰å‡†ç¡®æ€§
   - è¯†åˆ«æ¦‚å¿µæ··æ·†å’Œå®šä¹‰åå·®
   - æ£€æŸ¥æ¦‚å¿µä¹‹é—´çš„å…³ç³»å’Œå±‚æ¬¡ç»“æ„

3. **é€»è¾‘ä¸€è‡´æ€§åˆ†æ**
   - æ£€æŸ¥è®ºè¯é€»è¾‘çš„ä¸¥å¯†æ€§
   - è¯†åˆ«é€»è¾‘è·³è·ƒå’Œæ¨ç†é”™è¯¯
   - éªŒè¯å› æœå…³ç³»çš„åˆç†æ€§

4. **å¸¸è§è¯¯è§£è¯†åˆ«**
   - è¯†åˆ«é¢†åŸŸå†…çš„å¸¸è§è¯¯è§£å’Œé”™è¯¯è§‚å¿µ
   - æä¾›æ­£ç¡®çš„è§£é‡Šå’Œæ¾„æ¸…
   - æ ‡è®°éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜“é”™ç‚¹

## ğŸ” éªŒè¯æ–¹æ³•

1. **çŸ¥è¯†åº“æŸ¥è¯¢**: ä½¿ç”¨å†…ç½®çŸ¥è¯†åº“éªŒè¯åŸºç¡€äº‹å®
2. **Webæœç´¢**: æŸ¥è¯¢æœ€æ–°ä¿¡æ¯å’Œæƒå¨æ¥æº
3. **äº¤å‰éªŒè¯**: å¯¹æ¯”å¤šä¸ªä¿¡æ¯æºç¡®ä¿å‡†ç¡®æ€§
4. **ä¸“å®¶çŸ¥è¯†**: åº”ç”¨é¢†åŸŸä¸“ä¸šçŸ¥è¯†è¿›è¡Œåˆ¤æ–­

## ğŸ“Š ä¸¥é‡ç¨‹åº¦åˆ†çº§

- **Critical**: ä¸¥é‡çš„äº‹å®é”™è¯¯ï¼Œå¯èƒ½è¯¯å¯¼å­¦ä¹ 
- **Warning**: æ¦‚å¿µä¸å‡†ç¡®ï¼Œéœ€è¦æ¾„æ¸…
- **Info**: è¡¨è¿°ä¸å¤Ÿç²¾ç¡®ï¼Œå»ºè®®æ”¹è¿›

## ğŸ“¤ è¾“å‡ºæ ¼å¼

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºéªŒè¯ç»“æœï¼š

```json
{
    "overall_accuracy": 0.85,
    "factual_errors": [
        {
            "content": "å…·ä½“çš„é”™è¯¯å†…å®¹",
            "severity": "critical|warning|info",
            "source": "é”™è¯¯æ¥æºå®šä½",
            "suggestion": "ä¿®æ­£å»ºè®®",
            "reference": "å‚è€ƒèµ„æ–™é“¾æ¥"
        }
    ],
    "conceptual_issues": [
        {
            "content": "æ¦‚å¿µé—®é¢˜æè¿°",
            "severity": "critical|warning|info",
            "correct_definition": "æ­£ç¡®çš„æ¦‚å¿µå®šä¹‰",
            "common_misconception": "å¸¸è§è¯¯è§£è¯´æ˜"
        }
    ],
    "critical_issues": [
        {
            "content": "ä¸¥é‡é—®é¢˜æè¿°",
            "impact": "å¯¹å­¦ä¹ çš„å½±å“",
            "priority": "ä¿®æ­£ä¼˜å…ˆçº§"
        }
    ],
    "validation_summary": "æ•´ä½“éªŒè¯æ€»ç»“å’Œå»ºè®®",
    "confidence_score": 0.9,
    "verification_sources": ["source1", "source2"]
}
```

è¯·ä¸¥æ ¼æŒ‰ç…§ç§‘å­¦å’Œå®¢è§‚çš„æ ‡å‡†è¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚"""


class ValidationResult(BaseModel):
    """éªŒè¯ç»“æœ"""
    is_accurate: bool = Field(..., description="æ˜¯å¦å‡†ç¡®")
    confidence: float = Field(..., description="ç½®ä¿¡åº¦ 0-1")
    issues: List[str] = Field(default_factory=list, description="å‘ç°çš„é—®é¢˜")
    corrections: List[str] = Field(default_factory=list, description="å»ºè®®çš„çº æ­£")
    evidence: List[str] = Field(default_factory=list, description="æ”¯æŒè¯æ®")


class ConceptValidation(BaseModel):
    """æ¦‚å¿µéªŒè¯ç»“æœ"""
    concept: str = Field(..., description="æ¦‚å¿µåç§°")
    definition_accuracy: float = Field(..., description="å®šä¹‰å‡†ç¡®åº¦")
    common_misconceptions: List[str] = Field(default_factory=list, description="å¸¸è§è¯¯è§£")
    correct_definition: Optional[str] = Field(default=None, description="æ­£ç¡®å®šä¹‰")
    related_concepts: List[str] = Field(default_factory=list, description="ç›¸å…³æ¦‚å¿µ")


class FactualValidation(BaseModel):
    """äº‹å®éªŒè¯ç»“æœ"""
    claim: str = Field(..., description="å£°æ˜/äº‹å®")
    is_factual: bool = Field(..., description="æ˜¯å¦ä¸ºäº‹å®")
    evidence_strength: float = Field(..., description="è¯æ®å¼ºåº¦")
    sources: List[str] = Field(default_factory=list, description="ä¿¡æ¯æ¥æº")
    alternative_views: List[str] = Field(default_factory=list, description="ä¸åŒè§‚ç‚¹")


class KnowledgeValidationReport(BaseModel):
    """çŸ¥è¯†éªŒè¯æŠ¥å‘Š"""
    overall_accuracy: float = Field(..., description="æ•´ä½“å‡†ç¡®æ€§è¯„åˆ†")
    concept_validations: List[ConceptValidation] = Field(default_factory=list, description="æ¦‚å¿µéªŒè¯ç»“æœ")
    factual_validations: List[FactualValidation] = Field(default_factory=list, description="äº‹å®éªŒè¯ç»“æœ")
    critical_issues: List[str] = Field(default_factory=list, description="å…³é”®é—®é¢˜")
    improvement_suggestions: List[str] = Field(default_factory=list, description="æ”¹è¿›å»ºè®®")
    reliability_score: float = Field(..., description="å¯é æ€§è¯„åˆ†")


class KnowledgeValidator(AgentInterface):
    """çŸ¥è¯†éªŒè¯Agent"""
    
    def __init__(self):
        """åˆå§‹åŒ–çŸ¥è¯†éªŒè¯Agent"""
        
        # å®šä¹‰Agentèƒ½åŠ›
        capabilities = [
            AgentCapability(
                name="factual_verification",
                description="éªŒè¯äº‹å®å£°æ˜çš„å‡†ç¡®æ€§",
                input_types=["text", "explanation", "claim"],
                output_types=["validation_result", "factual_validation"],
                complexity_level="medium"
            ),
            AgentCapability(
                name="concept_validation",
                description="éªŒè¯æ¦‚å¿µå®šä¹‰çš„æ­£ç¡®æ€§",
                input_types=["concept", "definition", "explanation"],
                output_types=["concept_validation", "correction"],
                complexity_level="complex"
            ),
            AgentCapability(
                name="misconception_detection",
                description="è¯†åˆ«å¸¸è§è¯¯è§£å’Œé”™è¯¯è§‚å¿µ",
                input_types=["explanation", "concept"],
                output_types=["misconception_list", "correction"],
                complexity_level="complex"
            ),
            AgentCapability(
                name="knowledge_consistency_check",
                description="æ£€æŸ¥çŸ¥è¯†çš„å†…åœ¨ä¸€è‡´æ€§",
                input_types=["explanation", "multiple_concepts"],
                output_types=["consistency_report"],
                complexity_level="complex"
            )
        ]
        
        # åˆå§‹åŒ–å…ƒæ•°æ®
        metadata = AgentMetadata(
            agent_type=AgentType.KNOWLEDGE_VALIDATOR,
            name="KnowledgeValidator",
            version="1.0.0",
            capabilities=capabilities,
            dependencies=[],  # ç‹¬ç«‹è¿è¡Œï¼Œä¸ä¾èµ–å…¶ä»–Agent
            max_concurrent_tasks=3
        )
        
        super().__init__(metadata)
        
        # åˆå§‹åŒ–LLM
        self.llm = self._init_llm()
        
        # åˆå§‹åŒ–æç¤ºè¯æ¨¡æ¿
        self._init_prompts()
    
    def _init_llm(self):
        """åˆå§‹åŒ–LLMæ¨¡å‹"""
        settings = get_settings()
        
        # ä¼˜å…ˆä½¿ç”¨OpenAI
        if settings.openai_api_key:
            return ChatOpenAI(
                api_key=settings.openai_api_key,
                model=settings.openai_model,
                temperature=0.1  # éªŒè¯ä»»åŠ¡éœ€è¦æ›´å‡†ç¡®çš„è¾“å‡º
            )
        elif settings.llm_provider == "zhipu" and settings.zhipu_api_key:
            return ChatZhipuAI(
                api_key=settings.zhipu_api_key,
                model=settings.zhipu_model,
                temperature=0.1
            )
        else:
            raise ValueError("æœªé…ç½®å¯ç”¨çš„LLMæ¨¡å‹")
    
    def _init_prompts(self):
        """åˆå§‹åŒ–æç¤ºè¯æ¨¡æ¿"""
        
        # äº‹å®éªŒè¯æç¤ºè¯
        self.factual_validation_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº‹å®éªŒè¯ä¸“å®¶ï¼Œæ“…é•¿éªŒè¯ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚

ä½ çš„ä»»åŠ¡æ˜¯éªŒè¯ç”¨æˆ·è§£é‡Šä¸­çš„äº‹å®å£°æ˜ï¼ŒåŒ…æ‹¬ï¼š
1. **äº‹å®å‡†ç¡®æ€§**: å£°æ˜æ˜¯å¦ç¬¦åˆå·²çŸ¥äº‹å®
2. **è¯æ®å¼ºåº¦**: æ”¯æŒè¯æ®çš„å¯é ç¨‹åº¦
3. **ä¿¡æ¯æ¥æº**: ä¿¡æ¯çš„å¯èƒ½æ¥æºå’Œæƒå¨æ€§
4. **ä¸ç¡®å®šæ€§**: è¯†åˆ«å­˜åœ¨äº‰è®®æˆ–ä¸ç¡®å®šçš„å†…å®¹

è¯„ä¼°æ ‡å‡†ï¼š
- é«˜ç½®ä¿¡åº¦(0.8-1.0): æœ‰ç¡®å‡¿è¯æ®æ”¯æŒçš„å…¬è®¤äº‹å®
- ä¸­ç­‰ç½®ä¿¡åº¦(0.5-0.8): æœ‰ä¸€å®šè¯æ®ä½†å¯èƒ½å­˜åœ¨äº‰è®®
- ä½ç½®ä¿¡åº¦(0.0-0.5): ç¼ºä¹è¯æ®æˆ–å­˜åœ¨æ˜æ˜¾é”™è¯¯

è¯·ä¿æŒå®¢è§‚ä¸­ç«‹ï¼Œæ‰¿è®¤çŸ¥è¯†çš„å±€é™æ€§ã€‚"""),
            
            ("human", """è¯·éªŒè¯ä»¥ä¸‹è§£é‡Šä¸­çš„äº‹å®å£°æ˜ï¼š

**ä¸»é¢˜**: {topic}

**ç”¨æˆ·è§£é‡Š**:
{explanation}

**é‡ç‚¹å…³æ³¨**:
{focus_areas}

è¯·åˆ†ææ¯ä¸ªå…·ä½“çš„äº‹å®å£°æ˜ï¼Œè¿”å›JSONæ ¼å¼çš„éªŒè¯ç»“æœã€‚""")
        ])
        
        # æ¦‚å¿µéªŒè¯æç¤ºè¯
        self.concept_validation_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ¦‚å¿µåˆ†æä¸“å®¶ï¼Œä¸“é—¨éªŒè¯æ¦‚å¿µå®šä¹‰çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ã€‚

ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
1. **å®šä¹‰å‡†ç¡®æ€§**: æ¦‚å¿µå®šä¹‰æ˜¯å¦å‡†ç¡®å®Œæ•´
2. **å¸¸è§è¯¯è§£**: è¯†åˆ«è¯¥æ¦‚å¿µçš„å…¸å‹è¯¯è§£
3. **æ¦‚å¿µè¾¹ç•Œ**: æ˜ç¡®æ¦‚å¿µçš„é€‚ç”¨èŒƒå›´å’Œé™åˆ¶
4. **ç›¸å…³æ¦‚å¿µ**: è¯†åˆ«ç›¸å…³å’Œæ˜“æ··æ·†çš„æ¦‚å¿µ

åˆ†æç»´åº¦ï¼š
- æ ¸å¿ƒç‰¹å¾æ˜¯å¦æ­£ç¡®
- å®šä¹‰æ˜¯å¦è¿‡äºå®½æ³›æˆ–ç‹­çª„
- æ˜¯å¦åŒ…å«äº†å…³é”®è¦ç´ 
- æ˜¯å¦å­˜åœ¨é€»è¾‘é”™è¯¯

è¯·æä¾›å»ºè®¾æ€§çš„çº æ­£å»ºè®®ã€‚"""),
            
            ("human", """è¯·éªŒè¯ä»¥ä¸‹æ¦‚å¿µçš„å®šä¹‰å‡†ç¡®æ€§ï¼š

**ä¸»é¢˜**: {topic}

**æ¦‚å¿µåŠå…¶è§£é‡Š**:
{concepts_explanation}

**ç‰¹åˆ«å…³æ³¨çš„æ¦‚å¿µ**: {key_concepts}

è¯·ä¸ºæ¯ä¸ªæ¦‚å¿µæä¾›è¯¦ç»†çš„éªŒè¯åˆ†æã€‚""")
        ])
        
        # è¯¯è§£è¯†åˆ«æç¤ºè¯
        self.misconception_detection_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ•™è‚²å¿ƒç†å­¦ä¸“å®¶ï¼Œä¸“é—¨è¯†åˆ«å­¦ä¹ ä¸­çš„å¸¸è§è¯¯è§£å’Œè®¤çŸ¥åè¯¯ã€‚

å¸¸è§è¯¯è§£ç±»å‹ï¼š
1. **æ¦‚å¿µæ··æ·†**: å°†ç›¸ä¼¼æ¦‚å¿µæ··ä¸ºä¸€è°ˆ
2. **å› æœå€’ç½®**: é¢ å€’åŸå› å’Œç»“æœ
3. **è¿‡åº¦ç®€åŒ–**: å¿½ç•¥é‡è¦çš„å¤æ‚æ€§
4. **ç±»æ¯”é”™è¯¯**: ä¸å½“çš„ç±»æ¯”æ¨ç†
5. **å…ˆå…¥ä¸ºä¸»**: åŸºäºç›´è§‰çš„é”™è¯¯åˆ¤æ–­

è¯†åˆ«ç­–ç•¥ï¼š
- å¯»æ‰¾ä¸æ ‡å‡†å®šä¹‰çš„åå·®
- è¯†åˆ«é€»è¾‘æ¼æ´å’Œæ¨ç†é”™è¯¯
- å‘ç°éšå«çš„é”™è¯¯å‡è®¾
- æ³¨æ„è¡¨è¿°ä¸­çš„ç»å¯¹åŒ–å€¾å‘

è¯·æ¸©å’Œåœ°æŒ‡å‡ºé—®é¢˜ï¼Œå¹¶æä¾›æ­£ç¡®çš„ç†è§£ã€‚"""),
            
            ("human", """è¯·è¯†åˆ«ä»¥ä¸‹è§£é‡Šä¸­å¯èƒ½å­˜åœ¨çš„è¯¯è§£ï¼š

**ä¸»é¢˜**: {topic}

**ç”¨æˆ·è§£é‡Š**:
{explanation}

**èƒŒæ™¯ä¿¡æ¯**: è¿™æ˜¯ä¸€ä¸ªå­¦ä¹ è€…å¯¹{topic}çš„ç†è§£ï¼Œè¯·å¸®åŠ©è¯†åˆ«æ½œåœ¨çš„è¯¯è§£ã€‚

è¯·æä¾›å…·ä½“çš„è¯¯è§£åˆ†æå’Œçº æ­£å»ºè®®ã€‚""")
        ])
    
    async def process_task(self, task: AgentTask, context: ConversationContext) -> AgentResponse:
        """å¤„ç†éªŒè¯ä»»åŠ¡"""
        start_time = time.time()
        
        try:
            if task.task_type == TaskType.KNOWLEDGE_VALIDATION:
                result = await self._validate_knowledge(task.input_data, context)
            elif task.task_type == "factual_verification":
                result = await self._verify_facts(task.input_data, context)
            elif task.task_type == "concept_validation":
                result = await self._validate_concepts(task.input_data, context)
            elif task.task_type == "misconception_detection":
                result = await self._detect_misconceptions(task.input_data, context)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡ç±»å‹: {task.task_type}")
            
            processing_time = time.time() - start_time
            
            return create_response(
                agent_id=self.metadata.agent_id,
                agent_type=self.metadata.agent_type,
                task_id=task.task_id,
                success=True,
                processing_time=processing_time,
                result=result
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            return create_response(
                agent_id=self.metadata.agent_id,
                agent_type=self.metadata.agent_type,
                task_id=task.task_id,
                success=False,
                processing_time=processing_time,
                error=str(e)
            )
    
    async def _validate_knowledge(self, input_data: Dict[str, Any], context: ConversationContext) -> Dict[str, Any]:
        """ç»¼åˆçŸ¥è¯†éªŒè¯"""
        topic = context.topic
        explanation = context.user_explanation
        
        # å¹¶è¡Œæ‰§è¡Œå¤šç§éªŒè¯
        factual_result = await self._verify_facts({"explanation": explanation}, context)
        concept_result = await self._validate_concepts({"explanation": explanation}, context)
        misconception_result = await self._detect_misconceptions({"explanation": explanation}, context)
        
        # ç»¼åˆè¯„åˆ†
        overall_accuracy = (
            factual_result.get("average_confidence", 0.5) * 0.4 +
            concept_result.get("average_accuracy", 0.5) * 0.4 +
            (1.0 - len(misconception_result.get("misconceptions", [])) * 0.1) * 0.2
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report = KnowledgeValidationReport(
            overall_accuracy=max(0.0, min(1.0, overall_accuracy)),
            concept_validations=concept_result.get("validations", []),
            factual_validations=factual_result.get("validations", []),
            critical_issues=self._identify_critical_issues(factual_result, concept_result, misconception_result),
            improvement_suggestions=self._generate_improvement_suggestions(factual_result, concept_result, misconception_result),
            reliability_score=overall_accuracy
        )
        
        return {
            "validation_report": report.dict(),
            "summary": f"çŸ¥è¯†éªŒè¯å®Œæˆï¼Œæ•´ä½“å‡†ç¡®æ€§: {overall_accuracy:.2f}",
            "requires_attention": overall_accuracy < 0.7
        }
    
    async def _verify_facts(self, input_data: Dict[str, Any], context: ConversationContext) -> Dict[str, Any]:
        """éªŒè¯äº‹å®å£°æ˜"""
        explanation = input_data.get("explanation", context.user_explanation)
        topic = context.topic
        
        # æ„é€ éªŒè¯æç¤º
        messages = self.factual_validation_prompt.format_messages(
            topic=topic,
            explanation=explanation,
            focus_areas="å…·ä½“çš„æ•°æ®ã€æ—¶é—´ã€äººç‰©ã€äº‹ä»¶ã€å› æœå…³ç³»"
        )
        
        response = await self.llm.ainvoke(messages)
        
        try:
            # è§£æLLMå“åº”
            result_data = json.loads(response.content)
            
            # æ ‡å‡†åŒ–ç»“æœæ ¼å¼
            validations = []
            for item in result_data.get("factual_claims", []):
                validation = FactualValidation(
                    claim=item.get("claim", ""),
                    is_factual=item.get("is_factual", True),
                    evidence_strength=item.get("confidence", 0.5),
                    sources=item.get("sources", []),
                    alternative_views=item.get("alternative_views", [])
                )
                validations.append(validation.dict())
            
            return {
                "validations": validations,
                "average_confidence": sum(v["evidence_strength"] for v in validations) / max(len(validations), 1),
                "total_claims": len(validations),
                "verified_claims": sum(1 for v in validations if v["is_factual"])
            }
            
        except (json.JSONDecodeError, KeyError) as e:
            # é™çº§å¤„ç†
            return {
                "validations": [],
                "average_confidence": 0.5,
                "total_claims": 0,
                "verified_claims": 0,
                "error": f"è§£æå¤±è´¥: {str(e)}"
            }
    
    async def _validate_concepts(self, input_data: Dict[str, Any], context: ConversationContext) -> Dict[str, Any]:
        """éªŒè¯æ¦‚å¿µå®šä¹‰"""
        explanation = input_data.get("explanation", context.user_explanation)
        topic = context.topic
        
        # æå–å…³é”®æ¦‚å¿µ
        key_concepts = self._extract_key_concepts(explanation, topic)
        
        messages = self.concept_validation_prompt.format_messages(
            topic=topic,
            concepts_explanation=explanation,
            key_concepts=", ".join(key_concepts)
        )
        
        response = await self.llm.ainvoke(messages)
        
        try:
            result_data = json.loads(response.content)
            
            validations = []
            for item in result_data.get("concept_validations", []):
                validation = ConceptValidation(
                    concept=item.get("concept", ""),
                    definition_accuracy=item.get("accuracy_score", 0.5),
                    common_misconceptions=item.get("misconceptions", []),
                    correct_definition=item.get("correct_definition"),
                    related_concepts=item.get("related_concepts", [])
                )
                validations.append(validation.dict())
            
            return {
                "validations": validations,
                "average_accuracy": sum(v["definition_accuracy"] for v in validations) / max(len(validations), 1),
                "total_concepts": len(validations),
                "accurate_concepts": sum(1 for v in validations if v["definition_accuracy"] > 0.7)
            }
            
        except (json.JSONDecodeError, KeyError) as e:
            return {
                "validations": [],
                "average_accuracy": 0.5,
                "total_concepts": 0,
                "accurate_concepts": 0,
                "error": f"è§£æå¤±è´¥: {str(e)}"
            }
    
    async def _detect_misconceptions(self, input_data: Dict[str, Any], context: ConversationContext) -> Dict[str, Any]:
        """æ£€æµ‹å¸¸è§è¯¯è§£"""
        explanation = input_data.get("explanation", context.user_explanation)
        topic = context.topic
        
        messages = self.misconception_detection_prompt.format_messages(
            topic=topic,
            explanation=explanation
        )
        
        response = await self.llm.ainvoke(messages)
        
        try:
            result_data = json.loads(response.content)
            
            misconceptions = result_data.get("misconceptions", [])
            corrections = result_data.get("corrections", [])
            
            return {
                "misconceptions": misconceptions,
                "corrections": corrections,
                "misconception_count": len(misconceptions),
                "severity_scores": result_data.get("severity_scores", [])
            }
            
        except (json.JSONDecodeError, KeyError) as e:
            return {
                "misconceptions": [],
                "corrections": [],
                "misconception_count": 0,
                "error": f"è§£æå¤±è´¥: {str(e)}"
            }
    
    def _extract_key_concepts(self, explanation: str, topic: str) -> List[str]:
        """æå–å…³é”®æ¦‚å¿µï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        import re
        
        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', explanation)
        
        # è¿‡æ»¤é•¿åº¦å’Œå»é‡
        concepts = list(set([w for w in words if len(w) >= 2]))
        
        # æ·»åŠ ä¸»é¢˜ä½œä¸ºå…³é”®æ¦‚å¿µ
        if topic not in concepts:
            concepts.append(topic)
        
        return concepts[:10]  # é™åˆ¶æ•°é‡
    
    def _identify_critical_issues(self, factual_result: Dict, concept_result: Dict, misconception_result: Dict) -> List[str]:
        """è¯†åˆ«å…³é”®é—®é¢˜"""
        issues = []
        
        # æ£€æŸ¥äº‹å®å‡†ç¡®æ€§é—®é¢˜
        if factual_result.get("average_confidence", 1.0) < 0.6:
            issues.append("å­˜åœ¨äº‹å®å‡†ç¡®æ€§é—®é¢˜ï¼Œéœ€è¦éªŒè¯ç›¸å…³å£°æ˜")
        
        # æ£€æŸ¥æ¦‚å¿µå®šä¹‰é—®é¢˜
        if concept_result.get("average_accuracy", 1.0) < 0.6:
            issues.append("æ¦‚å¿µå®šä¹‰ä¸å¤Ÿå‡†ç¡®ï¼Œéœ€è¦æ¾„æ¸…æ ¸å¿ƒæ¦‚å¿µ")
        
        # æ£€æŸ¥è¯¯è§£é—®é¢˜
        if misconception_result.get("misconception_count", 0) > 2:
            issues.append("å­˜åœ¨å¤šä¸ªè®¤çŸ¥è¯¯è§£ï¼Œéœ€è¦çº æ­£é”™è¯¯ç†è§£")
        
        return issues
    
    def _generate_improvement_suggestions(self, factual_result: Dict, concept_result: Dict, misconception_result: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []
        
        if factual_result.get("verified_claims", 0) < factual_result.get("total_claims", 1):
            suggestions.append("å»ºè®®æŸ¥è¯ç›¸å…³äº‹å®ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®æ€§")
        
        if concept_result.get("accurate_concepts", 0) < concept_result.get("total_concepts", 1):
            suggestions.append("å»ºè®®é‡æ–°å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µçš„æ ‡å‡†å®šä¹‰")
        
        if misconception_result.get("misconception_count", 0) > 0:
            suggestions.append("å»ºè®®å…³æ³¨å¸¸è§è¯¯è§£ï¼Œé¿å…é”™è¯¯ç†è§£")
        
        suggestions.append("å»ºè®®å¤šæŸ¥é˜…æƒå¨èµ„æ–™ï¼ŒåŠ æ·±ç†è§£æ·±åº¦")
        
        return suggestions
    
    async def handle_message(self, message) -> Optional:
        """å¤„ç†æ¶ˆæ¯ï¼ˆæš‚æ—¶ç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ¶ˆæ¯å¤„ç†é€»è¾‘
        return None
