# è´¹æ›¼å­¦ä¹ ç³»ç»Ÿä»»åŠ¡é˜Ÿåˆ—ç°çŠ¶åˆ†æä¸å®æ–½æ–¹æ¡ˆ

## ğŸ“Š å½“å‰çŠ¶å†µè¯„ä¼°

### âŒ **ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿï¼šæœªå®ç°**

é€šè¿‡ä»£ç åˆ†æå‘ç°ï¼Œå½“å‰ç³»ç»Ÿ**æ²¡æœ‰å®ç°ä¸“ä¸šçš„ä»»åŠ¡é˜Ÿåˆ—ç³»ç»Ÿ**ï¼Œåªæœ‰åŸºç¡€çš„åå°ä»»åŠ¡å¤„ç†æœºåˆ¶ã€‚

## ğŸ” ç°æœ‰ä»»åŠ¡å¤„ç†æœºåˆ¶

### 1. å½“å‰å®ç°ï¼šFastAPI BackgroundTasks

```python
# api/routers/chat.py
@router.post("/chat", response_model=ChatResponse)
async def chat_with_agent(request: ChatRequest, background_tasks: BackgroundTasks):
    # ä¸»è¦ä¸šåŠ¡é€»è¾‘
    result = await langgraph_app.ainvoke(inputs, config)
    
    # ğŸ”´ ç®€å•åå°ä»»åŠ¡ï¼šè®°å¿†å›ºåŒ–
    background_tasks.add_task(
        summarize_conversation_for_memory,
        topic=request.topic,
        conversation_history=final_memory,
    )
    return response

@router.post("/memorize", status_code=202)
async def memorize_conversation(request: MemorizeRequest, background_tasks: BackgroundTasks):
    # ğŸ”´ ç®€å•åå°ä»»åŠ¡æ·»åŠ 
    background_tasks.add_task(
        summarize_conversation_for_memory,
        topic=request.topic,
        conversation_history=request.conversation_history,
    )
    return {"message": "è®°å¿†ä»»åŠ¡å·²åŠ å…¥åå°å¤„ç†é˜Ÿåˆ—ã€‚"}
```

### 2. ç›‘æ§é¢„ç•™æŒ‡æ ‡

```python
# core/monitoring/metrics.py
WORKFLOW_QUEUE_SIZE = Gauge(
    'langgraph_workflow_queue_size',  # ğŸŸ¡ é¢„ç•™æŒ‡æ ‡ï¼Œä½†æ— å®é™…é˜Ÿåˆ—
    'å·¥ä½œæµé˜Ÿåˆ—å¤§å°',
    registry=REGISTRY
)
```

## âš ï¸ å½“å‰æœºåˆ¶çš„å±€é™æ€§

### 1. **åŠŸèƒ½é™åˆ¶**
- âŒ æ— ä»»åŠ¡æŒä¹…åŒ–ï¼šæœåŠ¡é‡å¯ä»»åŠ¡ä¸¢å¤±
- âŒ æ— ä»»åŠ¡ä¼˜å…ˆçº§ï¼šå…ˆè¿›å…ˆå‡ºï¼Œæ— æ³•æ§åˆ¶å¤„ç†é¡ºåº
- âŒ æ— ä»»åŠ¡é‡è¯•ï¼šå¤±è´¥ä»»åŠ¡æ— æ³•è‡ªåŠ¨é‡è¯•
- âŒ æ— ä»»åŠ¡çŠ¶æ€è¿½è¸ªï¼šæ— æ³•æŸ¥è¯¢ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€
- âŒ æ— å¹¶å‘æ§åˆ¶ï¼šæ— æ³•é™åˆ¶åŒæ—¶æ‰§è¡Œçš„ä»»åŠ¡æ•°é‡

### 2. **å¯é æ€§é—®é¢˜**
- âŒ å†…å­˜ä¸­æ‰§è¡Œï¼šæœåŠ¡é‡å¯æˆ–å´©æºƒå¯¼è‡´ä»»åŠ¡ä¸¢å¤±
- âŒ æ— å¤±è´¥æ¢å¤ï¼šå¼‚å¸¸ä»»åŠ¡æ— æ³•æ¢å¤
- âŒ æ— ç›‘æ§èƒ½åŠ›ï¼šæ— æ³•ç›‘æ§ä»»åŠ¡æ‰§è¡Œæƒ…å†µ
- âŒ æ— è´Ÿè½½å‡è¡¡ï¼šæ— æ³•åˆ†å¸ƒå¼å¤„ç†ä»»åŠ¡

### 3. **æ‰©å±•æ€§ç“¶é¢ˆ**
- âŒ å•æœºé™åˆ¶ï¼šæ— æ³•è·¨æœåŠ¡å™¨åˆ†å¸ƒä»»åŠ¡
- âŒ æ— ä»»åŠ¡åˆ†ç±»ï¼šä¸åŒç±»å‹ä»»åŠ¡æ··åˆå¤„ç†
- âŒ æ— èµ„æºç®¡æ§ï¼šæ— æ³•æ§åˆ¶ä»»åŠ¡èµ„æºä½¿ç”¨

## ğŸ¯ ä»»åŠ¡é˜Ÿåˆ—éœ€æ±‚åˆ†æ

### 1. å½“å‰ä»»åŠ¡ç±»å‹

| ä»»åŠ¡ç±»å‹ | é¢‘ç‡ | é‡è¦æ€§ | ç‰¹ç‚¹ |
|----------|------|--------|------|
| **è®°å¿†å›ºåŒ–** | é«˜ | ä¸­ç­‰ | LLMè°ƒç”¨ï¼Œè€—æ—¶1-5s |
| **å¯¹è¯åˆ†æ** | é«˜ | é«˜ | å¤æ‚æ¨ç†ï¼Œè€—æ—¶2-10s |
| **å‘é‡å­˜å‚¨** | ä¸­ | ä¸­ç­‰ | I/Oæ“ä½œï¼Œè€—æ—¶0.5-2s |
| **æˆæœ¬ç»Ÿè®¡** | ä½ | ä½ | æ•°æ®å¤„ç†ï¼Œè€—æ—¶<1s |
| **ç›‘æ§ä¸ŠæŠ¥** | ä¸­ | ä½ | ç½‘ç»œè¯·æ±‚ï¼Œè€—æ—¶<1s |

### 2. ä¸šåŠ¡éœ€æ±‚

```python
# éœ€è¦æ”¯æŒçš„ä»»åŠ¡åœºæ™¯
TASK_SCENARIOS = {
    "immediate": {
        "name": "å®æ—¶ä»»åŠ¡",
        "examples": ["å¯¹è¯å“åº”", "å¥åº·æ£€æŸ¥"],
        "requirement": "åŒæ­¥å¤„ç†ï¼Œ<100ms"
    },
    "background": {
        "name": "åå°ä»»åŠ¡", 
        "examples": ["è®°å¿†å›ºåŒ–", "å‘é‡å­˜å‚¨"],
        "requirement": "å¼‚æ­¥å¤„ç†ï¼Œ5så†…å®Œæˆ"
    },
    "scheduled": {
        "name": "å®šæ—¶ä»»åŠ¡",
        "examples": ["æˆæœ¬ç»Ÿè®¡", "ç³»ç»Ÿæ¸…ç†"],
        "requirement": "å®šæ—¶æ‰§è¡Œï¼Œå¯å»¶è¿Ÿ"
    },
    "batch": {
        "name": "æ‰¹å¤„ç†ä»»åŠ¡",
        "examples": ["æ•°æ®å¯¼å‡º", "æ¨¡å‹è®­ç»ƒ"],
        "requirement": "å¤§æ•°æ®é‡ï¼Œé•¿æ—¶é—´è¿è¡Œ"
    }
}
```

## ğŸ—ï¸ ä»»åŠ¡é˜Ÿåˆ—å®æ–½æ–¹æ¡ˆ

### æ–¹æ¡ˆAï¼šè½»é‡çº§æ”¹è¿› (æ¨èèµ·æ­¥)

#### åŸºäº asyncio.Queue çš„å†…å­˜é˜Ÿåˆ—

```python
# core/task_queue/async_queue.py
import asyncio
import uuid
import time
from enum import Enum
from typing import Dict, Any, Callable, Optional
from dataclasses import dataclass, field
from datetime import datetime

class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRY = "retry"

@dataclass
class Task:
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    name: str = ""
    func: Callable = None
    args: tuple = field(default_factory=tuple)
    kwargs: Dict[str, Any] = field(default_factory=dict)
    priority: int = 0  # æ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜
    max_retries: int = 3
    retry_count: int = 0
    status: TaskStatus = TaskStatus.PENDING
    created_at: datetime = field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    result: Any = None
    error: Optional[str] = None

class AsyncTaskQueue:
    def __init__(self, max_workers: int = 5):
        self.max_workers = max_workers
        self.task_queue = asyncio.PriorityQueue()
        self.tasks: Dict[str, Task] = {}
        self.workers: list = []
        self.running = False
        self.semaphore = asyncio.Semaphore(max_workers)
        
    async def start(self):
        """å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—å¤„ç†å™¨"""
        self.running = True
        self.workers = [
            asyncio.create_task(self._worker(f"worker-{i}"))
            for i in range(self.max_workers)
        ]
        
    async def stop(self):
        """åœæ­¢ä»»åŠ¡é˜Ÿåˆ—å¤„ç†å™¨"""
        self.running = False
        for worker in self.workers:
            worker.cancel()
        await asyncio.gather(*self.workers, return_exceptions=True)
        
    async def add_task(
        self, 
        func: Callable,
        *args,
        name: str = "",
        priority: int = 0,
        max_retries: int = 3,
        **kwargs
    ) -> str:
        """æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        task = Task(
            name=name or func.__name__,
            func=func,
            args=args,
            kwargs=kwargs,
            priority=priority,
            max_retries=max_retries
        )
        
        self.tasks[task.id] = task
        # ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼šè´Ÿå€¼ä½¿é«˜ä¼˜å…ˆçº§ä»»åŠ¡å…ˆæ‰§è¡Œ
        await self.task_queue.put((-task.priority, task.created_at, task.id))
        
        return task.id
        
    async def get_task_status(self, task_id: str) -> Optional[Task]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.tasks.get(task_id)
        
    async def _worker(self, worker_name: str):
        """å·¥ä½œçº¿ç¨‹å¤„ç†ä»»åŠ¡"""
        while self.running:
            try:
                # è·å–ä»»åŠ¡
                _, _, task_id = await self.task_queue.get()
                task = self.tasks.get(task_id)
                
                if not task:
                    continue
                    
                # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
                async with self.semaphore:
                    await self._execute_task(task, worker_name)
                    
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"Worker {worker_name} error: {e}")
                
    async def _execute_task(self, task: Task, worker_name: str):
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task.status = TaskStatus.RUNNING
            task.started_at = datetime.now()
            
            # æ‰§è¡Œä»»åŠ¡
            if asyncio.iscoroutinefunction(task.func):
                result = await task.func(*task.args, **task.kwargs)
            else:
                result = await asyncio.get_event_loop().run_in_executor(
                    None, task.func, *task.args
                )
                
            # ä»»åŠ¡æˆåŠŸ
            task.status = TaskStatus.COMPLETED
            task.result = result
            task.completed_at = datetime.now()
            
            print(f"[{worker_name}] Task {task.name} completed successfully")
            
        except Exception as e:
            # ä»»åŠ¡å¤±è´¥
            task.error = str(e)
            
            if task.retry_count < task.max_retries:
                # é‡è¯•ä»»åŠ¡
                task.retry_count += 1
                task.status = TaskStatus.RETRY
                await self.task_queue.put((-task.priority, datetime.now(), task.id))
                print(f"[{worker_name}] Task {task.name} failed, retrying ({task.retry_count}/{task.max_retries})")
            else:
                # é‡è¯•æ¬¡æ•°è€—å°½
                task.status = TaskStatus.FAILED
                task.completed_at = datetime.now()
                print(f"[{worker_name}] Task {task.name} failed permanently: {e}")

# å…¨å±€ä»»åŠ¡é˜Ÿåˆ—å®ä¾‹
task_queue_manager = AsyncTaskQueue(max_workers=5)
```

#### é›†æˆåˆ°FastAPIåº”ç”¨

```python
# main.py
from core.task_queue.async_queue import task_queue_manager

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶ï¼šåˆå§‹åŒ–ä»»åŠ¡é˜Ÿåˆ—"""
    await task_queue_manager.start()
    logger.info("å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—å·²å¯åŠ¨")

@app.on_event("shutdown") 
async def shutdown_event():
    """å…³é—­äº‹ä»¶ï¼šåœæ­¢ä»»åŠ¡é˜Ÿåˆ—"""
    await task_queue_manager.stop()
    logger.info("å¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—å·²åœæ­¢")
```

#### æ›´æ–°APIè·¯ç”±

```python
# api/routers/chat.py
from core.task_queue.async_queue import task_queue_manager

@router.post("/chat", response_model=ChatResponse)
async def chat_with_agent(request: ChatRequest):
    # ä¸»è¦ä¸šåŠ¡é€»è¾‘
    result = await langgraph_app.ainvoke(inputs, config)
    
    # âœ… ä½¿ç”¨ä»»åŠ¡é˜Ÿåˆ—ï¼šè®°å¿†å›ºåŒ–
    task_id = await task_queue_manager.add_task(
        summarize_conversation_for_memory,
        topic=request.topic,
        conversation_history=result.get("short_term_memory", []),
        name="memory_consolidation",
        priority=5  # ä¸­ç­‰ä¼˜å…ˆçº§
    )
    
    return ChatResponse(
        questions=result.get("question_queue", []),
        session_id=request.session_id,
        short_term_memory=result.get("short_term_memory", []),
        background_task_id=task_id  # è¿”å›ä»»åŠ¡ID
    )

@router.get("/task/{task_id}")
async def get_task_status(task_id: str):
    """æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€"""
    task = await task_queue_manager.get_task_status(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return {
        "task_id": task.id,
        "name": task.name,
        "status": task.status.value,
        "created_at": task.created_at.isoformat(),
        "started_at": task.started_at.isoformat() if task.started_at else None,
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
        "retry_count": task.retry_count,
        "error": task.error
    }
```

### æ–¹æ¡ˆBï¼šRedis + Celery (æ¨èç”Ÿäº§ç¯å¢ƒ)

#### ä¾èµ–å®‰è£…

```bash
# requirements.txt æ–°å¢
redis>=4.5.0
celery>=5.3.0
flower>=2.0.0  # Celeryç›‘æ§é¢æ¿
```

#### Celeryé…ç½®

```python
# core/task_queue/celery_app.py
from celery import Celery
from celery.signals import task_prerun, task_postrun
import os

# Celeryåº”ç”¨åˆå§‹åŒ–
celery_app = Celery(
    "feynman_tasks",
    broker=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
    backend=os.getenv("REDIS_URL", "redis://localhost:6379/0"),
    include=["core.task_queue.tasks"]
)

# Celeryé…ç½®
celery_app.conf.update(
    # ä»»åŠ¡è·¯ç”±
    task_routes={
        "memory.*": {"queue": "memory"},
        "analysis.*": {"queue": "analysis"},
        "monitoring.*": {"queue": "monitoring"}
    },
    
    # ä»»åŠ¡ä¼˜å…ˆçº§
    task_default_priority=5,
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    
    # ç»“æœè¿‡æœŸæ—¶é—´
    result_expires=3600,
    
    # æ—¶åŒºè®¾ç½®
    timezone="Asia/Shanghai",
    enable_utc=True,
    
    # åºåˆ—åŒ–
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    
    # ç›‘æ§
    worker_send_task_events=True,
    task_send_sent_event=True,
)

# ä»»åŠ¡ç›‘æ§é’©å­
@task_prerun.connect
def task_prerun_handler(signal=None, sender=None, task_id=None, task=None, args=None, kwargs=None, **kwds):
    """ä»»åŠ¡å¼€å§‹æ‰§è¡Œå‰çš„é’©å­"""
    print(f"Task {task.name}[{task_id}] starting...")

@task_postrun.connect  
def task_postrun_handler(signal=None, sender=None, task_id=None, task=None, args=None, kwargs=None, retval=None, state=None, **kwds):
    """ä»»åŠ¡æ‰§è¡Œå®Œæˆåçš„é’©å­"""
    print(f"Task {task.name}[{task_id}] finished with state: {state}")
```

#### ä»»åŠ¡å®šä¹‰

```python
# core/task_queue/tasks.py
from .celery_app import celery_app
from agent.agent import summarize_conversation_for_memory
from core.monitoring.metrics import record_llm_usage
import asyncio

@celery_app.task(
    name="memory.consolidate_conversation",
    bind=True,
    autoretry_for=(Exception,),
    retry_kwargs={'max_retries': 3, 'countdown': 60},
    priority=5
)
def consolidate_conversation_memory(self, topic: str, conversation_history: list):
    """è®°å¿†å›ºåŒ–ä»»åŠ¡"""
    try:
        # è¿è¡Œå¼‚æ­¥å‡½æ•°
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        result = loop.run_until_complete(
            summarize_conversation_for_memory(topic, conversation_history)
        )
        
        return {"status": "success", "result": result}
        
    except Exception as exc:
        # è®°å½•å¤±è´¥æŒ‡æ ‡
        print(f"Memory consolidation failed: {exc}")
        raise self.retry(exc=exc)
    finally:
        loop.close()

@celery_app.task(
    name="analysis.process_user_feedback", 
    priority=8  # é«˜ä¼˜å…ˆçº§
)
def process_user_feedback(session_id: str, feedback_data: dict):
    """ç”¨æˆ·åé¦ˆåˆ†æä»»åŠ¡"""
    # å¤„ç†ç”¨æˆ·åé¦ˆé€»è¾‘
    return {"status": "processed", "session_id": session_id}

@celery_app.task(
    name="monitoring.collect_metrics",
    priority=1  # ä½ä¼˜å…ˆçº§
)
def collect_system_metrics():
    """ç³»ç»ŸæŒ‡æ ‡æ”¶é›†ä»»åŠ¡"""
    # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
    return {"status": "collected"}
```

#### FastAPIé›†æˆ

```python
# api/routers/chat.py  
from core.task_queue.tasks import consolidate_conversation_memory

@router.post("/chat", response_model=ChatResponse)
async def chat_with_agent(request: ChatRequest):
    result = await langgraph_app.ainvoke(inputs, config)
    
    # âœ… ä½¿ç”¨Celeryä»»åŠ¡é˜Ÿåˆ—
    task = consolidate_conversation_memory.delay(
        topic=request.topic,
        conversation_history=result.get("short_term_memory", [])
    )
    
    return ChatResponse(
        questions=result.get("question_queue", []),
        session_id=request.session_id,
        short_term_memory=result.get("short_term_memory", []),
        background_task_id=task.id
    )

@router.get("/task/{task_id}")
async def get_celery_task_status(task_id: str):
    """æŸ¥è¯¢Celeryä»»åŠ¡çŠ¶æ€"""
    from core.task_queue.celery_app import celery_app
    
    result = celery_app.AsyncResult(task_id)
    
    return {
        "task_id": task_id,
        "status": result.status,
        "result": result.result,
        "info": result.info,
        "date_done": result.date_done,
        "traceback": result.traceback
    }
```

#### å¯åŠ¨è„šæœ¬

```bash
# scripts/start_celery.sh
#!/bin/bash

# å¯åŠ¨Celery Worker
celery -A core.task_queue.celery_app worker \
  --loglevel=info \
  --concurrency=4 \
  --queues=memory,analysis,monitoring \
  --hostname=worker@%h

# å¯åŠ¨Celery Beat (å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨)
celery -A core.task_queue.celery_app beat --loglevel=info

# å¯åŠ¨Flower (ç›‘æ§é¢æ¿)
celery -A core.task_queue.celery_app flower --port=5555
```

### æ–¹æ¡ˆCï¼šRQ (Redis Queue) - è½»é‡çº§é€‰æ‹©

```python
# core/task_queue/rq_queue.py
import redis
from rq import Queue, Worker, Job
from rq.job import JobStatus
import os

# Redisè¿æ¥
redis_conn = redis.Redis(
    host=os.getenv("REDIS_HOST", "localhost"),
    port=int(os.getenv("REDIS_PORT", 6379)),
    db=int(os.getenv("REDIS_DB", 0))
)

# ä¸åŒä¼˜å…ˆçº§çš„é˜Ÿåˆ—
high_queue = Queue("high", connection=redis_conn)    # é«˜ä¼˜å…ˆçº§
default_queue = Queue("default", connection=redis_conn)  # é»˜è®¤ä¼˜å…ˆçº§  
low_queue = Queue("low", connection=redis_conn)      # ä½ä¼˜å…ˆçº§

class RQTaskManager:
    def __init__(self):
        self.queues = {
            "high": high_queue,
            "default": default_queue, 
            "low": low_queue
        }
    
    def add_task(self, func, *args, priority="default", job_timeout="5m", **kwargs):
        """æ·»åŠ ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        queue = self.queues.get(priority, default_queue)
        job = queue.enqueue(
            func,
            *args,
            **kwargs,
            job_timeout=job_timeout,
            retry=3
        )
        return job.id
    
    def get_task_status(self, job_id: str):
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        job = Job.fetch(job_id, connection=redis_conn)
        return {
            "id": job.id,
            "status": job.get_status(),
            "result": job.result,
            "exc_info": job.exc_info,
            "created_at": job.created_at,
            "started_at": job.started_at,
            "ended_at": job.ended_at
        }

# å…¨å±€RQç®¡ç†å™¨
rq_manager = RQTaskManager()
```

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | FastAPI BackgroundTasks | asyncio.Queue | Celery + Redis | RQ + Redis |
|------|-------------------------|---------------|----------------|------------|
| **å®ç°å¤æ‚åº¦** | ä½ | ä¸­ | é«˜ | ä¸­ |
| **åŠŸèƒ½å®Œæ•´æ€§** | åŸºç¡€ | ä¸­ç­‰ | å®Œæ•´ | ä¸­ç­‰ |
| **ä»»åŠ¡æŒä¹…åŒ–** | âŒ | âŒ | âœ… | âœ… |
| **åˆ†å¸ƒå¼æ”¯æŒ** | âŒ | âŒ | âœ… | âœ… |
| **ä»»åŠ¡ç›‘æ§** | âŒ | åŸºç¡€ | å®Œæ•´ | åŸºç¡€ |
| **æ•…éšœæ¢å¤** | âŒ | âŒ | âœ… | âœ… |
| **æ€§èƒ½å¼€é”€** | æœ€ä½ | ä½ | ä¸­ç­‰ | ä½ |
| **å­¦ä¹ æˆæœ¬** | æœ€ä½ | ä½ | é«˜ | ä¸­ |
| **é€‚ç”¨åœºæ™¯** | æ¼”ç¤º/æµ‹è¯• | å¼€å‘/å°é¡¹ç›® | ç”Ÿäº§/å¤§é¡¹ç›® | ä¸­å°é¡¹ç›® |

## ğŸ¯ æ¨èå®æ–½è·¯çº¿

### Phase 1: ç«‹å³å®æ–½ (æ–¹æ¡ˆA)
- âœ… å®ç°åŸºäº asyncio.Queue çš„å†…å­˜ä»»åŠ¡é˜Ÿåˆ—
- âœ… æ”¯æŒä»»åŠ¡ä¼˜å…ˆçº§å’Œé‡è¯•æœºåˆ¶  
- âœ… æ·»åŠ ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢API
- âœ… é›†æˆç›‘æ§æŒ‡æ ‡

### Phase 2: ç”Ÿäº§å‡†å¤‡ (æ–¹æ¡ˆBæˆ–C)
- ğŸ”„ å¼•å…¥Redisä½œä¸ºä»»åŠ¡å­˜å‚¨
- ğŸ”„ å®ç°ä»»åŠ¡æŒä¹…åŒ–å’Œåˆ†å¸ƒå¼å¤„ç†
- ğŸ”„ æ·»åŠ ä»»åŠ¡ç›‘æ§é¢æ¿
- ğŸ”„ æ”¯æŒå®šæ—¶ä»»åŠ¡è°ƒåº¦

### Phase 3: é«˜çº§åŠŸèƒ½
- ğŸ“‹ ä»»åŠ¡åˆ†ç»„å’Œæ‰¹é‡å¤„ç†
- ğŸ“‹ ä»»åŠ¡ä¾èµ–å…³ç³»ç®¡ç†
- ğŸ“‹ ä»»åŠ¡ç»“æœç¼“å­˜
- ğŸ“‹ ä»»åŠ¡æ‰§è¡Œåˆ†æå’Œä¼˜åŒ–

## ğŸš€ å³æ—¶è¡ŒåŠ¨æ–¹æ¡ˆ

æ ¹æ®å½“å‰éœ€æ±‚ï¼Œå»ºè®®**ç«‹å³å®æ–½æ–¹æ¡ˆA**ï¼š

1. **åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—æ¨¡å—** (é¢„è®¡2å°æ—¶)
2. **é›†æˆåˆ°ç°æœ‰API** (é¢„è®¡1å°æ—¶)  
3. **æ·»åŠ ç›‘æ§æŒ‡æ ‡** (é¢„è®¡1å°æ—¶)
4. **ç¼–å†™æµ‹è¯•ç”¨ä¾‹** (é¢„è®¡1å°æ—¶)

è¿™æ ·å¯ä»¥åœ¨**åŠå¤©å†…**ä»æ— é˜Ÿåˆ—ç³»ç»Ÿå‡çº§åˆ°å…·å¤‡åŸºç¡€åŠŸèƒ½çš„ä»»åŠ¡é˜Ÿåˆ—ï¼Œæ˜¾è‘—æå‡ç³»ç»Ÿçš„å¯é æ€§å’Œå¯è§‚æµ‹æ€§ã€‚

---

**è¯„ä¼°ç»“è®º**: å½“å‰ç³»ç»Ÿç¼ºä¹ä¸“ä¸šçš„ä»»åŠ¡é˜Ÿåˆ—å®ç°ï¼Œå»ºè®®ä¼˜å…ˆå®æ–½è½»é‡çº§å¼‚æ­¥é˜Ÿåˆ—æ–¹æ¡ˆï¼Œä¸ºåç»­ç”Ÿäº§éƒ¨ç½²æ‰“ä¸‹åŸºç¡€ã€‚
