# è´¹æ›¼å­¦ä¹ ç³»ç»ŸWebå¼‚æ­¥æ¡†æ¶æ¶æ„åˆ†æ

## ğŸ“– æ¡†æ¶æ¦‚è¿°

è´¹æ›¼å­¦ä¹ ç³»ç»Ÿé‡‡ç”¨ç°ä»£åŒ–çš„**å¼‚æ­¥Webæ¶æ„**ï¼ŒåŸºäºFastAPIæ„å»ºé«˜æ€§èƒ½çš„APIæœåŠ¡ï¼Œé›†æˆäº†å®Œæ•´çš„ç›‘æ§ã€è¿½è¸ªå’Œè§‚æµ‹èƒ½åŠ›ã€‚ç³»ç»Ÿæ”¯æŒæµå¼å“åº”ã€å¹¶å‘å¤„ç†å’Œæ™ºèƒ½å·¥ä½œæµç¼–æ’ã€‚

## ğŸ—ï¸ æ•´ä½“æ¶æ„

### æ ¸å¿ƒæŠ€æœ¯æ ˆ

| å±‚çº§ | æŠ€æœ¯é€‰æ‹© | ç”¨é€” |
|------|----------|------|
| **Webæ¡†æ¶** | FastAPI 3.2 | å¼‚æ­¥APIæœåŠ¡ã€è‡ªåŠ¨æ–‡æ¡£ç”Ÿæˆ |
| **å‰ç«¯UI** | Streamlit | å¿«é€ŸåŸå‹å’Œæ¼”ç¤ºç•Œé¢ |
| **å·¥ä½œæµå¼•æ“** | LangGraph | AI Agentå·¥ä½œæµç¼–æ’ |
| **å¼‚æ­¥è¿è¡Œæ—¶** | Asyncio + Uvicorn | é«˜æ€§èƒ½å¼‚æ­¥å¤„ç† |
| **ç›‘æ§è§‚æµ‹** | OpenTelemetry + Prometheus | åˆ†å¸ƒå¼è¿½è¸ªå’ŒæŒ‡æ ‡æ”¶é›† |

### æ¶æ„ç‰¹ç‚¹

1. **ğŸš€ å…¨å¼‚æ­¥è®¾è®¡**: ä»HTTPå¤„ç†åˆ°AIæ¨ç†å…¨é“¾è·¯å¼‚æ­¥
2. **ğŸ“Š å®Œæ•´è§‚æµ‹æ€§**: Metrics + Logging + Tracing ä¸‰å¤§æ”¯æŸ±
3. **ğŸ”€ æµå¼å¤„ç†**: æ”¯æŒå®æ—¶æµå¼å“åº”å’Œé•¿è¿æ¥
4. **ğŸ›¡ï¸ å¥å£®æ€§**: å¤šå±‚ä¸­é—´ä»¶ä¿æŠ¤å’Œé”™è¯¯æ¢å¤
5. **ğŸ“ˆ é«˜æ€§èƒ½**: å¹¶å‘å¤„ç†å’Œèµ„æºä¼˜åŒ–

## ğŸ“± å®¢æˆ·ç«¯å±‚è®¾è®¡

### 1. Streamlitå‰ç«¯ (`ui.py`)

```python
# æµå¼APIè°ƒç”¨
def stream_chat_api(topic: str, explanation: str, session_id: str, memory: List[Dict]):
    """å¼‚æ­¥æµå¼å¯¹è¯æ¥å£"""
    with requests.post(API_URL, json=payload, stream=True, timeout=300) as response:
        for line in response.iter_lines():
            # å¤„ç†Server-Sent Events
            if decoded_line.startswith('data: '):
                content = json.loads(content_json)
                yield content
```

**ç‰¹æ€§**:
- âœ… æµå¼å“åº”å¤„ç†
- âœ… ä¼šè¯çŠ¶æ€ç®¡ç†
- âœ… å®æ—¶æ‰“å­—æœºæ•ˆæœ
- âœ… é”™è¯¯æ¢å¤æœºåˆ¶

### 2. ç›´æ¥APIè®¿é—®

```bash
# RESTful APIç«¯ç‚¹
POST /chat          # æ ‡å‡†å¯¹è¯æ¥å£
POST /chat/stream   # æµå¼å¯¹è¯æ¥å£  
POST /memorize      # è®°å¿†å›ºåŒ–æ¥å£
GET  /health        # å¥åº·æ£€æŸ¥
GET  /metrics       # ç›‘æ§æŒ‡æ ‡
```

## ğŸŒ ç½‘å…³å±‚æ¶æ„

### 1. FastAPIåº”ç”¨æ ¸å¿ƒ (`main.py`)

```python
app = FastAPI(
    title="Feynman Student Agent API",
    description="åŸºäºè´¹æ›¼å­¦ä¹ æ³•çš„AIå­¦ç”ŸAgent",
    version="3.2"
)

# ä¸­é—´ä»¶æ ˆï¼ˆæ‰§è¡Œé¡ºåºï¼šåè¿›å…ˆå‡ºï¼‰
app.add_middleware(RequestTimeoutMiddleware)      # è¯·æ±‚è¶…æ—¶æ§åˆ¶
app.add_middleware(RequestSizeLimitMiddleware)    # è¯·æ±‚å¤§å°é™åˆ¶  
app.add_middleware(MonitoringMiddleware)          # ç›‘æ§æ•°æ®æ”¶é›†

# è·¯ç”±æ³¨å†Œ
app.include_router(chat_router, tags=["å¯¹è¯"])
app.include_router(monitoring_router, tags=["ç›‘æ§"])
```

### 2. ä¸­é—´ä»¶æ ˆè®¾è®¡

#### MonitoringMiddleware - ç›‘æ§ä¸­é—´ä»¶
```python
class MonitoringMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next: Callable):
        # 1. ç”Ÿæˆè¯·æ±‚IDå’Œä¸Šä¸‹æ–‡
        request_id = str(uuid.uuid4())
        set_request_context(request_id=request_id, session_id=session_id)
        
        # 2. è®°å½•å¼€å§‹æ—¶é—´å’Œæ´»è·ƒè¿æ¥
        start_time = time.time()
        API_ACTIVE_CONNECTIONS.inc()
        
        # 3. å¤„ç†è¯·æ±‚
        response = await call_next(request)
        duration = time.time() - start_time
        
        # 4. æ”¶é›†æŒ‡æ ‡å’Œæ—¥å¿—
        self._record_metrics(request, response, duration)
        self._log_request(request, response, duration)
        
        # 5. ç‰¹æ®Šå¤„ç†æµå¼å“åº”
        if self._is_streaming_response(response):
            response = await self._wrap_streaming_response(response, request)
            
        return response
```

**ç›‘æ§èƒ½åŠ›**:
- ğŸ“Š APIè°ƒç”¨æŒ‡æ ‡ (å»¶è¿Ÿã€QPSã€é”™è¯¯ç‡)
- ğŸ“ ç»“æ„åŒ–è¯·æ±‚æ—¥å¿—  
- ğŸ” åˆ†å¸ƒå¼è¯·æ±‚è¿½è¸ª
- ğŸŒŠ æµå¼è¿æ¥ç›‘æ§
- ğŸ’° æˆæœ¬ä½¿ç”¨ç»Ÿè®¡

#### RequestTimeoutMiddleware - è¶…æ—¶æ§åˆ¶
```python
class RequestTimeoutMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next: Callable):
        try:
            return await asyncio.wait_for(
                call_next(request), 
                timeout=self.timeout_seconds
            )
        except asyncio.TimeoutError:
            return JSONResponse(
                status_code=408,
                content={"detail": f"è¯·æ±‚è¶…æ—¶ ({self.timeout_seconds}s)"}
            )
```

#### RequestSizeLimitMiddleware - å¤§å°é™åˆ¶
```python
class RequestSizeLimitMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next: Callable):
        content_length = request.headers.get("content-length")
        if content_length and int(content_length) > self.max_size:
            return JSONResponse(
                status_code=413,
                content={"detail": f"è¯·æ±‚è¿‡å¤§ (æœ€å¤§: {self.max_size}å­—èŠ‚)"}
            )
        return await call_next(request)
```

## ğŸ›£ï¸ è·¯ç”±å±‚è®¾è®¡

### 1. å¯¹è¯è·¯ç”± (`api/routers/chat.py`)

#### æ ‡å‡†å¯¹è¯æ¥å£
```python
@router.post("/chat", response_model=ChatResponse)
async def chat_with_agent(request: ChatRequest, background_tasks: BackgroundTasks):
    inputs = {
        "topic": request.topic,
        "user_explanation": request.explanation,
        "short_term_memory": request.short_term_memory,
    }
    config = {"configurable": {"thread_id": request.session_id}}

    # å¼‚æ­¥æ‰§è¡ŒLangGraphå·¥ä½œæµ
    result = await langgraph_app.ainvoke(inputs, config)
    
    # åå°ä»»åŠ¡ï¼šè®°å¿†å›ºåŒ–
    background_tasks.add_task(
        summarize_conversation_for_memory,
        topic=request.topic,
        conversation_history=result.get("short_term_memory", [])
    )
    
    return ChatResponse(
        questions=result.get("question_queue", []),
        session_id=request.session_id,
        short_term_memory=result.get("short_term_memory", [])
    )
```

#### æµå¼å¯¹è¯æ¥å£
```python
@router.post("/chat/stream")
async def chat_with_agent_stream(request: ChatRequest):
    # æµå¼å“åº”ç”Ÿæˆå™¨
    async def stream_generator(app, inputs: Dict, config: Dict):
        # ç›‘å¬LangGraphäº‹ä»¶æµ
        async for event in app.astream_events(inputs, config, version="v1"):
            if event["event"] == "on_chat_model_stream":
                content = event["data"]["chunk"].content
                if content:
                    yield f"data: {json.dumps(content)}\n\n"
                    await asyncio.sleep(0.05)  # æ§åˆ¶æµé€Ÿ
        
        yield f"data: {json.dumps('[END_OF_STREAM]')}\n\n"
    
    return StreamingResponse(
        stream_generator(langgraph_app, inputs, config), 
        media_type="text/event-stream"
    )
```

**ç‰¹æ€§**:
- âš¡ å¼‚æ­¥å¤„ç†ï¼šéé˜»å¡å¹¶å‘æ‰§è¡Œ
- ğŸŒŠ æµå¼å“åº”ï¼šServer-Sent Events
- ğŸ”„ åå°ä»»åŠ¡ï¼šè®°å¿†å›ºåŒ–å¼‚æ­¥å¤„ç†
- ğŸ›¡ï¸ é”™è¯¯å¤„ç†ï¼šç»Ÿä¸€å¼‚å¸¸ç®¡ç†

### 2. ç›‘æ§è·¯ç”± (`api/routers/monitoring.py`)

```python
@router.get("/health")
async def health_check():
    """å¤šç»´åº¦å¥åº·æ£€æŸ¥"""
    health_data = await health_checker.run_all_checks()
    
    # æ£€æŸ¥é¡¹ç›®ï¼š
    # - æ•°æ®åº“è¿æ¥
    # - å¤–éƒ¨APIå¯ç”¨æ€§  
    # - ç³»ç»Ÿèµ„æºçŠ¶æ€
    # - ä¾èµ–æœåŠ¡å¥åº·
    
    status_code = 200 if health_data["status"] == "healthy" else 503
    return JSONResponse(content=health_data, status_code=status_code)

@router.get("/metrics")
async def get_metrics():
    """PrometheusæŒ‡æ ‡å¯¼å‡º"""
    registry = get_registry()
    metrics_collector.collect_system_metrics()  # å®æ—¶é‡‡é›†
    return Response(
        generate_latest(registry), 
        media_type=CONTENT_TYPE_LATEST
    )
```

## ğŸ§  ä¸šåŠ¡é€»è¾‘å±‚

### 1. LangGraphå·¥ä½œæµå¼•æ“

```python
# æ„å»ºå¼‚æ­¥å·¥ä½œæµ
def build_graph():
    workflow = StateGraph(AgentState)
    
    # èŠ‚ç‚¹å®šä¹‰ï¼ˆæ”¯æŒå¹¶è¡Œæ‰§è¡Œï¼‰
    workflow.add_node("user_input_handler", user_input_handler)
    workflow.add_node("gap_identifier", gap_identifier_react)  
    workflow.add_node("question_generator", question_generator)
    
    # æµç¨‹ç¼–æ’
    workflow.set_entry_point("user_input_handler")
    workflow.add_edge("user_input_handler", "gap_identifier")
    workflow.add_edge("gap_identifier", "question_generator")
    workflow.add_edge("question_generator", END)
    
    return workflow.compile()

# å¼‚æ­¥å·¥ä½œæµæ‰§è¡Œ
@monitor_workflow_node("gap_identifier_react")
@trace_langchain_workflow("gap_identifier_react")
async def gap_identifier_react(state: AgentState) -> AgentState:
    """ReAct Agentå¼‚æ­¥æ¨ç†"""
    with trace_span("react_agent_execution"):
        # å¼‚æ­¥æ‰§è¡ŒReActæ¨ç†
        agent_output = await react_agent_executor.ainvoke({
            "messages": state.get("messages", [])
        })
        
        # è§£æç»“æ„åŒ–è¾“å‡º
        analysis_result = AgentOutputParser.parse_agent_output(
            agent_output['messages'][-1].content
        )
        
    return {
        "unclear_points": analysis_result.unclear_points,
        "_analysis_result": analysis_result.dict()
    }
```

### 2. å·¥å…·è°ƒç”¨å±‚å¼‚æ­¥åŒ–

```python
# å¼‚æ­¥å·¥å…·è°ƒç”¨ç¤ºä¾‹
@tool
async def async_web_search(query: str) -> str:
    """å¼‚æ­¥ç½‘ç»œæœç´¢å·¥å…·"""
    async with aiohttp.ClientSession() as session:
        async with session.post(API_URL, json={"query": query}) as response:
            return await response.text()

# å¹¶è¡Œå·¥å…·è°ƒç”¨
async def parallel_tool_calls():
    """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥å…·"""
    tasks = [
        async_web_search("Pythonç¼–ç¨‹"),
        async_translate("Hello World"),
        async_calculate("2 + 3 * 4")
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

## ğŸ’¾ æ•°æ®æŒä¹…åŒ–å±‚

### 1. å¼‚æ­¥æ•°æ®æ“ä½œ

```python
# å¼‚æ­¥è®°å¿†å­˜å‚¨
@trace_memory_operation("summarize_and_store")
async def summarize_conversation_for_memory(topic: str, conversation_history: List[Dict]):
    """å¼‚æ­¥è®°å¿†å›ºåŒ–"""
    if not conversation_history:
        return
    
    # å¼‚æ­¥LLMè°ƒç”¨ç”Ÿæˆæ‘˜è¦
    conversation_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history])
    
    if MODEL_AVAILABLE and model:
        summary = await model.ainvoke({"conversation_str": conversation_str})
    else:
        summary = f"ç®€åŒ–æ‘˜è¦: ä¸»é¢˜={topic}, å¯¹è¯è½®æ•°={len(conversation_history)}"
    
    # å¼‚æ­¥å‘é‡å­˜å‚¨
    await asyncio.get_event_loop().run_in_executor(
        None, 
        memory_manager_instance.add_memory, 
        summary.content,
        {"topic": topic, "timestamp": datetime.now().isoformat()}
    )
```

### 2. å‘é‡æ•°æ®åº“é›†æˆ

```python
# ChromaDBå¼‚æ­¥æ“ä½œå°è£…
class AsyncChromaManager:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./chroma_db")
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def similarity_search(self, query: str, k: int = 5):
        """å¼‚æ­¥å‘é‡ç›¸ä¼¼æ€§æœç´¢"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self._sync_similarity_search,
            query, k
        )
```

## ğŸ“Š ç›‘æ§è§‚æµ‹å±‚

### 1. åˆ†å¸ƒå¼è¿½è¸ª

```python
# OpenTelemetryè¿½è¸ªé›†æˆ
@trace_langchain_workflow("conversation_flow")
async def process_conversation(request: ChatRequest):
    with trace_span("input_validation") as span:
        span.set_attribute("topic", request.topic)
        span.set_attribute("explanation_length", len(request.explanation))
        
        # åµŒå¥—è¿½è¸ª
        with trace_span("workflow_execution"):
            result = await langgraph_app.ainvoke(inputs, config)
            
            # è®°å½•ä¸šåŠ¡æŒ‡æ ‡
            span.set_attribute("questions_generated", len(result.get("question_queue", [])))
            span.set_attribute("processing_time_ms", duration * 1000)
```

### 2. æŒ‡æ ‡æ”¶é›†

```python
# Prometheuså¼‚æ­¥æŒ‡æ ‡æ”¶é›†
class AsyncMetricsCollector:
    def __init__(self):
        # è®¡æ•°å™¨
        self.api_requests_total = Counter(
            'api_requests_total',
            'Total API requests',
            ['method', 'endpoint', 'status']
        )
        
        # ç›´æ–¹å›¾
        self.request_duration = Histogram(
            'api_request_duration_seconds',
            'Request duration in seconds',
            ['method', 'endpoint']
        )
        
        # ä»ªè¡¨ç›˜
        self.active_connections = Gauge(
            'api_active_connections',
            'Number of active connections'
        )
    
    async def record_request(self, method: str, endpoint: str, status: int, duration: float):
        """å¼‚æ­¥è®°å½•è¯·æ±‚æŒ‡æ ‡"""
        self.api_requests_total.labels(
            method=method, 
            endpoint=endpoint, 
            status=status
        ).inc()
        
        self.request_duration.labels(
            method=method, 
            endpoint=endpoint
        ).observe(duration)
```

### 3. ç»“æ„åŒ–æ—¥å¿—

```python
# å¼‚æ­¥ç»“æ„åŒ–æ—¥å¿—
import structlog

logger = structlog.get_logger("api.async")

async def log_async_operation(operation: str, **context):
    """å¼‚æ­¥æ“ä½œæ—¥å¿—è®°å½•"""
    await logger.ainfo(
        f"å¼‚æ­¥æ“ä½œ: {operation}",
        operation=operation,
        timestamp=datetime.now().isoformat(),
        trace_id=get_trace_id(),
        **context
    )
```

## ğŸ”„ å¼‚æ­¥å¤„ç†ç‰¹æ€§

### 1. å¹¶å‘æ§åˆ¶

```python
# ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
semaphore = asyncio.Semaphore(10)  # æœ€å¤§10ä¸ªå¹¶å‘

async def rate_limited_operation():
    async with semaphore:
        # é™åˆ¶å¹¶å‘æ‰§è¡Œ
        result = await expensive_operation()
        return result

# ä»»åŠ¡é˜Ÿåˆ—
task_queue = asyncio.Queue(maxsize=100)

async def worker():
    """åå°å·¥ä½œçº¿ç¨‹"""
    while True:
        task = await task_queue.get()
        try:
            await process_task(task)
        finally:
            task_queue.task_done()
```

### 2. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10)
)
async def resilient_api_call(url: str, data: dict):
    """å…·æœ‰é‡è¯•æœºåˆ¶çš„APIè°ƒç”¨"""
    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=data) as response:
            if response.status >= 400:
                raise aiohttp.ClientError(f"APIé”™è¯¯: {response.status}")
            return await response.json()
```

### 3. æµå¼å¤„ç†

```python
# å¼‚æ­¥ç”Ÿæˆå™¨ç”¨äºæµå¼å“åº”
async def stream_llm_response(prompt: str):
    """æµå¼LLMå“åº”ç”Ÿæˆå™¨"""
    async with openai_client.chat.completions.create(
        model="gpt-4",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    ) as stream:
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                await asyncio.sleep(0.01)  # æ§åˆ¶æµé€Ÿ

# Server-Sent Eventså“åº”
async def sse_response_generator():
    """SSEå¼‚æ­¥å“åº”ç”Ÿæˆå™¨"""
    async for token in stream_llm_response(prompt):
        yield f"data: {json.dumps({'token': token})}\n\n"
    
    yield f"data: {json.dumps({'done': True})}\n\n"
```

## âš¡ æ€§èƒ½ä¼˜åŒ–

### 1. è¿æ¥æ± ç®¡ç†

```python
# HTTPå®¢æˆ·ç«¯è¿æ¥æ± 
class AsyncHTTPClient:
    def __init__(self):
        self.session = aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(
                limit=100,           # æ€»è¿æ¥æ•°é™åˆ¶
                limit_per_host=10,   # æ¯ä¸ªä¸»æœºè¿æ¥æ•°é™åˆ¶
                ttl_dns_cache=300,   # DNSç¼“å­˜TTL
                use_dns_cache=True,
            )
        )
    
    async def close(self):
        await self.session.close()

# æ•°æ®åº“è¿æ¥æ± 
import asyncpg

class AsyncDBPool:
    def __init__(self):
        self.pool = None
    
    async def init_pool(self):
        self.pool = await asyncpg.create_pool(
            dsn=DATABASE_URL,
            min_size=5,
            max_size=20,
            command_timeout=60
        )
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# å¼‚æ­¥ç¼“å­˜
import aioredis
from functools import wraps

class AsyncCache:
    def __init__(self):
        self.redis = None
    
    async def init_redis(self):
        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
        self.redis = await aioredis.from_url(redis_url)
    
    def cached(self, ttl: int = 300):
        """å¼‚æ­¥ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
                
                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = await self.redis.get(cache_key)
                if cached_result:
                    return json.loads(cached_result)
                
                # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
                result = await func(*args, **kwargs)
                await self.redis.setex(cache_key, ttl, json.dumps(result))
                return result
            return wrapper
        return decorator

# ä½¿ç”¨ç¤ºä¾‹
@cache_manager.cached(ttl=600)
async def expensive_computation(param: str):
    # è€—æ—¶è®¡ç®—
    await asyncio.sleep(2)
    return f"ç»“æœ: {param}"
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### å½“å‰æ€§èƒ½è¡¨ç°

| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |
|------|------|------|
| **APIå“åº”æ—¶é—´** | P95 < 2s | 95%è¯·æ±‚åœ¨2ç§’å†…å“åº” |
| **å¹¶å‘è¿æ¥æ•°** | 1000+ | æ”¯æŒ1000+å¹¶å‘è¿æ¥ |
| **ååé‡** | 500+ RPS | æ¯ç§’å¤„ç†500+è¯·æ±‚ |
| **å†…å­˜ä½¿ç”¨** | < 512MB | åŸºç¡€å†…å­˜å ç”¨ |
| **æµå¼å»¶è¿Ÿ** | < 100ms | é¦–å­—èŠ‚å“åº”æ—¶é—´ |

### æ€§èƒ½ç›‘æ§æŒ‡æ ‡

```python
# å…³é”®æ€§èƒ½æŒ‡æ ‡
PERFORMANCE_METRICS = {
    "api_latency_p95": "APIå»¶è¿Ÿ95åˆ†ä½æ•°",
    "api_latency_p99": "APIå»¶è¿Ÿ99åˆ†ä½æ•°", 
    "concurrent_requests": "å¹¶å‘è¯·æ±‚æ•°",
    "request_rate": "è¯·æ±‚é€Ÿç‡ (RPS)",
    "error_rate": "é”™è¯¯ç‡",
    "memory_usage": "å†…å­˜ä½¿ç”¨ç‡",
    "cpu_usage": "CPUä½¿ç”¨ç‡",
    "active_connections": "æ´»è·ƒè¿æ¥æ•°",
    "stream_connections": "æµå¼è¿æ¥æ•°",
    "llm_response_time": "LLMå“åº”æ—¶é—´",
    "database_query_time": "æ•°æ®åº“æŸ¥è¯¢æ—¶é—´"
}
```

## ğŸ”® æœªæ¥å‘å±•æ–¹å‘

### Phase 1: å½“å‰æ¶æ„ âœ…
- FastAPI + LangGraphå¼‚æ­¥æ¶æ„
- åŸºç¡€ç›‘æ§å’Œè§‚æµ‹èƒ½åŠ›
- æµå¼å“åº”æ”¯æŒ
- å¤šå·¥å…·é›†æˆ

### Phase 2: æ€§èƒ½ä¼˜åŒ– (è¿›è¡Œä¸­)
- æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
- Redisç¼“å­˜å±‚
- è´Ÿè½½å‡è¡¡æ”¯æŒ
- WebSocketå®æ—¶é€šä¿¡

### Phase 3: å¾®æœåŠ¡åŒ– (è§„åˆ’ä¸­)
- æœåŠ¡æ‹†åˆ†å’Œè§£è€¦
- APIç½‘å…³é›†æˆ
- æœåŠ¡å‘ç°å’Œæ³¨å†Œ
- å®¹å™¨åŒ–éƒ¨ç½²

### Phase 4: äº‘åŸç”Ÿ (é•¿æœŸ)
- Kubernetesç¼–æ’
- è‡ªåŠ¨æ‰©ç¼©å®¹
- å¤šåŒºåŸŸéƒ¨ç½²
- è¾¹ç¼˜è®¡ç®—æ”¯æŒ

## ğŸ›¡ï¸ å¯é æ€§ä¿è¯

### 1. é”™è¯¯æ¢å¤æœºåˆ¶

```python
# å…¨å±€å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"æœªå¤„ç†å¼‚å¸¸: {str(exc)}", extra={
        "exception_type": type(exc).__name__,
        "path": str(request.url.path),
        "method": request.method,
        "trace_id": get_trace_id()
    })
    
    return JSONResponse(
        status_code=500,
        content={"detail": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"}
    )
```

### 2. ç†”æ–­å™¨æ¨¡å¼

```python
# ç†”æ–­å™¨å®ç°
class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
    
    async def call(self, func, *args, **kwargs):
        if self.state == "OPEN":
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
            else:
                raise Exception("ç†”æ–­å™¨å¼€å¯ï¼ŒæœåŠ¡ä¸å¯ç”¨")
        
        try:
            result = await func(*args, **kwargs)
            if self.state == "HALF_OPEN":
                self.state = "CLOSED"
                self.failure_count = 0
            return result
        except Exception as e:
            self.failure_count += 1
            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                self.last_failure_time = time.time()
            raise e
```

### 3. ä¼˜é›…å…³é—­

```python
# åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
@app.on_event("shutdown")
async def graceful_shutdown():
    """ä¼˜é›…å…³é—­å¤„ç†"""
    logger.info("å¼€å§‹ä¼˜é›…å…³é—­...")
    
    # ç­‰å¾…æ´»è·ƒè¯·æ±‚å®Œæˆ
    while API_ACTIVE_CONNECTIONS._value._value > 0:
        logger.info(f"ç­‰å¾… {API_ACTIVE_CONNECTIONS._value._value} ä¸ªè¯·æ±‚å®Œæˆ...")
        await asyncio.sleep(1)
    
    # å…³é—­æ•°æ®åº“è¿æ¥
    await db_pool.close()
    
    # å…³é—­HTTPå®¢æˆ·ç«¯
    await http_client.close()
    
    logger.info("ä¼˜é›…å…³é—­å®Œæˆ")
```

## ğŸ“š æ€»ç»“

è´¹æ›¼å­¦ä¹ ç³»ç»Ÿçš„Webå¼‚æ­¥æ¡†æ¶å…·å¤‡ä»¥ä¸‹æ ¸å¿ƒä¼˜åŠ¿ï¼š

### ğŸ¯ æ¶æ„ä¼˜åŠ¿
1. **ç°ä»£åŒ–è®¾è®¡**: åŸºäºFastAPIçš„å…¨å¼‚æ­¥æ¶æ„
2. **é«˜æ€§èƒ½**: æ”¯æŒå¤§è§„æ¨¡å¹¶å‘å’Œæµå¼å¤„ç†  
3. **å¯è§‚æµ‹**: å®Œæ•´çš„ç›‘æ§ã€è¿½è¸ªã€æ—¥å¿—ä½“ç³»
4. **å¯æ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
5. **å¥å£®æ€§**: å¤šå±‚é˜²æŠ¤å’Œé”™è¯¯æ¢å¤æœºåˆ¶

### ğŸš€ æŠ€æœ¯ç‰¹ç‚¹
- **å¼‚æ­¥å¤„ç†**: éé˜»å¡I/Oå’Œå¹¶å‘æ‰§è¡Œ
- **æµå¼å“åº”**: å®æ—¶æ•°æ®æµå’Œé•¿è¿æ¥æ”¯æŒ
- **æ™ºèƒ½ç¼–æ’**: LangGraphå·¥ä½œæµè‡ªåŠ¨åŒ–
- **å·¥å…·é›†æˆ**: å¤šç§å¤–éƒ¨APIçš„å¼‚æ­¥è°ƒç”¨
- **çŠ¶æ€ç®¡ç†**: åˆ†å¸ƒå¼ä¼šè¯å’Œè®°å¿†ç®¡ç†

### ğŸ“ˆ ä¸šåŠ¡ä»·å€¼
- **ç”¨æˆ·ä½“éªŒ**: å¿«é€Ÿå“åº”å’Œå®æ—¶äº¤äº’
- **ç³»ç»Ÿç¨³å®š**: é«˜å¯ç”¨å’Œæ•…éšœè‡ªæ„ˆèƒ½åŠ›  
- **è¿ç»´æ•ˆç‡**: å…¨æ–¹ä½ç›‘æ§å’Œè‡ªåŠ¨åŒ–è¿ç»´
- **æˆæœ¬æ§åˆ¶**: èµ„æºä¼˜åŒ–å’Œæ™ºèƒ½è°ƒåº¦
- **æœªæ¥å°±ç»ª**: äº‘åŸç”Ÿå’Œå¾®æœåŠ¡æ¶æ„

è¯¥æ¶æ„ä¸ºè´¹æ›¼å­¦ä¹ ç³»ç»Ÿæä¾›äº†åšå®çš„æŠ€æœ¯åŸºç¡€ï¼Œæ”¯æŒé«˜å¹¶å‘ã€ä½å»¶è¿Ÿçš„AIå¯¹è¯æœåŠ¡ï¼ŒåŒæ—¶å…·å¤‡ç”Ÿäº§çº§çš„å¯é æ€§å’Œå¯è§‚æµ‹æ€§ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: v3.2.0  
**æœ€åæ›´æ–°**: 2024å¹´8æœˆ21æ—¥  
**æ¶æ„è´Ÿè´£**: AI Assistant Team

