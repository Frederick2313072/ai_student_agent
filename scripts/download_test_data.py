"""
ä¸‹è½½çœŸå®æ•°æ®ç”¨äºçŸ¥è¯†å›¾è°±å¤§è§„æ¨¡æµ‹è¯•

ä»å¤šä¸ªæ•°æ®æºä¸‹è½½æ–‡æœ¬æ•°æ®ï¼š
1. ç»´åŸºç™¾ç§‘æ–‡ç« 
2. æ–°é—»æ–‡ç«   
3. å­¦æœ¯è®ºæ–‡æ‘˜è¦
4. æŠ€æœ¯æ–‡æ¡£
"""

import os
import requests
import json
import time
from typing import List, Dict
import urllib.parse


def download_wikipedia_articles(topics: List[str], lang: str = "zh") -> List[Dict[str, str]]:
    """ä¸‹è½½ç»´åŸºç™¾ç§‘æ–‡ç« """
    articles = []
    
    print(f"ğŸ“š æ­£åœ¨ä¸‹è½½ {len(topics)} ä¸ªç»´åŸºç™¾ç§‘ä¸»é¢˜...")
    
    for topic in topics:
        try:
            # ç»´åŸºç™¾ç§‘API
            wiki_api = f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{urllib.parse.quote(topic)}"
            
            response = requests.get(wiki_api, timeout=10)
            if response.status_code == 200:
                data = response.json()
                
                # è·å–å®Œæ•´æ–‡ç« å†…å®¹
                if data.get("extract"):
                    articles.append({
                        "title": data.get("title", topic),
                        "content": data.get("extract", ""),
                        "source": f"wikipedia_{lang}",
                        "url": data.get("content_urls", {}).get("desktop", {}).get("page", "")
                    })
                    print(f"  âœ… {topic}")
                else:
                    print(f"  âš ï¸ {topic} - å†…å®¹ä¸ºç©º")
            else:
                print(f"  âŒ {topic} - HTTP {response.status_code}")
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            print(f"  âŒ {topic} - é”™è¯¯: {e}")
    
    return articles


def download_tech_articles() -> List[Dict[str, str]]:
    """ä¸‹è½½æŠ€æœ¯æ–‡ç« å†…å®¹"""
    articles = []
    
    # ä¸€äº›æŠ€æœ¯æ¦‚å¿µçš„é¢„å®šä¹‰å†…å®¹
    tech_content = {
        "äººå·¥æ™ºèƒ½": """
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
        äººå·¥æ™ºèƒ½åŒ…å«å¤šä¸ªå­é¢†åŸŸï¼Œå¦‚æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰ã€‚
        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
        è‡ªç„¶è¯­è¨€å¤„ç†ä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
        è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œåˆ†æå›¾åƒå’Œè§†é¢‘å†…å®¹ã€‚
        """,
        "åŒºå—é“¾æŠ€æœ¯": """
        åŒºå—é“¾æ˜¯ä¸€ç§åˆ†å¸ƒå¼æ•°æ®åº“æŠ€æœ¯ï¼Œé€šè¿‡å¯†ç å­¦ç¡®ä¿æ•°æ®çš„å®‰å…¨æ€§å’Œä¸å¯ç¯¡æ”¹æ€§ã€‚
        åŒºå—é“¾ç”±å¤šä¸ªåŒºå—ç»„æˆï¼Œæ¯ä¸ªåŒºå—åŒ…å«äº¤æ˜“è®°å½•å’Œå‰ä¸€ä¸ªåŒºå—çš„å“ˆå¸Œå€¼ã€‚
        æ¯”ç‰¹å¸æ˜¯ç¬¬ä¸€ä¸ªæˆåŠŸåº”ç”¨åŒºå—é“¾æŠ€æœ¯çš„åŠ å¯†è´§å¸ã€‚
        ä»¥å¤ªåŠåœ¨æ¯”ç‰¹å¸åŸºç¡€ä¸Šå¢åŠ äº†æ™ºèƒ½åˆçº¦åŠŸèƒ½ã€‚
        æ™ºèƒ½åˆçº¦æ˜¯è‡ªåŠ¨æ‰§è¡Œçš„åˆçº¦ï¼Œå…¶æ¡æ¬¾ç›´æ¥å†™å…¥ä»£ç ä¸­ã€‚
        å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰æ˜¯åŸºäºåŒºå—é“¾çš„é‡‘èæœåŠ¡ã€‚
        """,
        "äº‘è®¡ç®—": """
        äº‘è®¡ç®—æ˜¯é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—èµ„æºå’ŒæœåŠ¡çš„æŠ€æœ¯æ¨¡å¼ã€‚
        äº‘è®¡ç®—æœåŠ¡æ¨¡å‹åŒ…æ‹¬åŸºç¡€è®¾æ–½å³æœåŠ¡ï¼ˆIaaSï¼‰ã€å¹³å°å³æœåŠ¡ï¼ˆPaaSï¼‰å’Œè½¯ä»¶å³æœåŠ¡ï¼ˆSaaSï¼‰ã€‚
        Amazon Web Servicesï¼ˆAWSï¼‰æ˜¯å…¨çƒæœ€å¤§çš„äº‘æœåŠ¡æä¾›å•†ã€‚
        Microsoft Azureå’ŒGoogle Cloud Platformæ˜¯AWSçš„ä¸»è¦ç«äº‰å¯¹æ‰‹ã€‚
        å®¹å™¨åŒ–æŠ€æœ¯å¦‚Dockeræ”¹å˜äº†åº”ç”¨ç¨‹åºçš„éƒ¨ç½²æ–¹å¼ã€‚
        Kubernetesæ˜¯å®¹å™¨ç¼–æ’çš„äº‹å®æ ‡å‡†ã€‚
        å¾®æœåŠ¡æ¶æ„å°†åº”ç”¨æ‹†åˆ†ä¸ºå¤šä¸ªå°å‹æœåŠ¡ã€‚
        """,
        "æ•°æ®ç§‘å­¦": """
        æ•°æ®ç§‘å­¦æ˜¯åˆ©ç”¨ç»Ÿè®¡å­¦ã€æœºå™¨å­¦ä¹ å’Œç¼–ç¨‹æŠ€èƒ½ä»æ•°æ®ä¸­æå–æ´å¯Ÿçš„è·¨å­¦ç§‘é¢†åŸŸã€‚
        Pythonå’ŒRæ˜¯æ•°æ®ç§‘å­¦ä¸­æœ€æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ã€‚
        Pandasæ˜¯Pythonä¸­æœ€é‡è¦çš„æ•°æ®å¤„ç†åº“ã€‚
        NumPyä¸ºPythonæä¾›äº†é«˜æ•ˆçš„æ•°ç»„è®¡ç®—åŠŸèƒ½ã€‚
        Matplotlibå’ŒSeabornç”¨äºæ•°æ®å¯è§†åŒ–ã€‚
        Jupyter Notebookæ˜¯æ•°æ®ç§‘å­¦å®¶å¸¸ç”¨çš„äº¤äº’å¼å¼€å‘ç¯å¢ƒã€‚
        æœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚
        """,
        "ç½‘ç»œå®‰å…¨": """
        ç½‘ç»œå®‰å…¨æ˜¯ä¿æŠ¤è®¡ç®—æœºç³»ç»Ÿã€ç½‘ç»œå’Œæ•°æ®å…å—æ”»å‡»çš„å®è·µã€‚
        é˜²ç«å¢™æ˜¯ç½‘ç»œå®‰å…¨çš„ç¬¬ä¸€é“é˜²çº¿ï¼Œæ§åˆ¶ç½‘ç»œæµé‡ã€‚
        åŠ å¯†æŠ€æœ¯ä¿æŠ¤æ•°æ®åœ¨ä¼ è¾“å’Œå­˜å‚¨è¿‡ç¨‹ä¸­çš„å®‰å…¨ã€‚
        å…¥ä¾µæ£€æµ‹ç³»ç»Ÿï¼ˆIDSï¼‰ç›‘æ§ç½‘ç»œå¼‚å¸¸æ´»åŠ¨ã€‚
        æ¸—é€æµ‹è¯•é€šè¿‡æ¨¡æ‹Ÿæ”»å‡»æ¥å‘ç°å®‰å…¨æ¼æ´ã€‚
        æ¶æ„è½¯ä»¶åŒ…æ‹¬ç—…æ¯’ã€è •è™«ã€ç‰¹æ´›ä¼Šæœ¨é©¬å’Œå‹’ç´¢è½¯ä»¶ã€‚
        èº«ä»½è®¤è¯å’Œè®¿é—®æ§åˆ¶ç¡®ä¿åªæœ‰æˆæƒç”¨æˆ·èƒ½è®¿é—®ç³»ç»Ÿã€‚
        """
    }
    
    print("ğŸ“– å‡†å¤‡æŠ€æœ¯æ–‡ç« å†…å®¹...")
    
    for title, content in tech_content.items():
        articles.append({
            "title": title,
            "content": content.strip(),
            "source": "tech_articles",
            "url": ""
        })
        print(f"  âœ… {title}")
    
    return articles


def download_news_articles() -> List[Dict[str, str]]:
    """ä¸‹è½½æ–°é—»æ–‡ç« ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰"""
    articles = []
    
    # ç§‘æŠ€æ–°é—»æ¨¡æ‹Ÿå†…å®¹
    news_content = {
        "ChatGPTæ¨åŠ¨AIé©å‘½": """
        OpenAIå‘å¸ƒçš„ChatGPTæ ‡å¿—ç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„é‡å¤§çªç ´ã€‚
        ChatGPTåŸºäºTransformeræ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ã€‚
        å¤§è¯­è¨€æ¨¡å‹é€šè¿‡æµ·é‡æ–‡æœ¬æ•°æ®è®­ç»ƒè·å¾—è¯­è¨€ç†è§£èƒ½åŠ›ã€‚
        GPT-4æ˜¯OpenAIæœ€æ–°çš„å¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚
        Googleæ¨å‡ºäº†Bardä½œä¸ºChatGPTçš„ç«äº‰å¯¹æ‰‹ã€‚
        ç™¾åº¦å‘å¸ƒäº†æ–‡å¿ƒä¸€è¨€å‚ä¸å¤§æ¨¡å‹ç«èµ›ã€‚
        """,
        "å…ƒå®‡å®™æŠ€æœ¯å‘å±•": """
        å…ƒå®‡å®™æ˜¯è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®æŠ€æœ¯çš„èåˆåº”ç”¨ã€‚
        Metaï¼ˆåŸFacebookï¼‰å¤§åŠ›æŠ•èµ„å…ƒå®‡å®™åŸºç¡€è®¾æ–½å»ºè®¾ã€‚
        è™šæ‹Ÿç°å®ï¼ˆVRï¼‰æä¾›æ²‰æµ¸å¼çš„ä¸‰ç»´ä½“éªŒã€‚
        å¢å¼ºç°å®ï¼ˆARï¼‰å°†è™šæ‹Ÿä¿¡æ¯å åŠ åˆ°ç°å®ä¸–ç•Œã€‚
        åŒºå—é“¾æŠ€æœ¯ä¸ºå…ƒå®‡å®™æä¾›æ•°å­—èµ„äº§ç¡®æƒã€‚
        NFTï¼ˆéåŒè´¨åŒ–ä»£å¸ï¼‰æˆä¸ºæ•°å­—è‰ºæœ¯å“çš„æ–°è½½ä½“ã€‚
        """,
        "é‡å­è®¡ç®—è¿›å±•": """
        é‡å­è®¡ç®—åˆ©ç”¨é‡å­åŠ›å­¦åŸç†è¿›è¡Œä¿¡æ¯å¤„ç†ã€‚
        é‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰æ˜¯é‡å­è®¡ç®—çš„åŸºæœ¬å•ä½ã€‚
        IBMã€Googleå’Œä¸­ç§‘é™¢éƒ½åœ¨é‡å­è®¡ç®—é¢†åŸŸå–å¾—é‡è¦è¿›å±•ã€‚
        é‡å­ä¼˜åŠ¿æ˜¯æŒ‡é‡å­è®¡ç®—æœºåœ¨ç‰¹å®šé—®é¢˜ä¸Šè¶…è¶Šç»å…¸è®¡ç®—æœºã€‚
        é‡å­çº ç¼ å’Œé‡å­å åŠ æ˜¯é‡å­è®¡ç®—çš„æ ¸å¿ƒåŸç†ã€‚
        é‡å­ç®—æ³•å¦‚Shorç®—æ³•å¯ä»¥å¨èƒç°æœ‰çš„åŠ å¯†ç³»ç»Ÿã€‚
        """
    }
    
    print("ğŸ“° å‡†å¤‡æ–°é—»æ–‡ç« å†…å®¹...")
    
    for title, content in news_content.items():
        articles.append({
            "title": title,
            "content": content.strip(),
            "source": "news",
            "url": ""
        })
        print(f"  âœ… {title}")
    
    return articles


def save_articles_to_files(articles: List[Dict[str, str]], data_dir: str = "data/test_corpus"):
    """ä¿å­˜æ–‡ç« åˆ°æ–‡ä»¶"""
    os.makedirs(data_dir, exist_ok=True)
    
    print(f"ğŸ’¾ ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ° {data_dir}...")
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "total_articles": len(articles),
        "sources": list(set(article["source"] for article in articles)),
        "download_time": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open(os.path.join(data_dir, "metadata.json"), 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    # ä¿å­˜æ¯ç¯‡æ–‡ç« 
    for i, article in enumerate(articles):
        filename = f"{i:03d}_{article['title'].replace('/', '_')[:50]}.txt"
        filepath = os.path.join(data_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {article['title']}\n")
            f.write(f"æ¥æº: {article['source']}\n")
            f.write(f"URL: {article['url']}\n")
            f.write(f"{'='*50}\n\n")
            f.write(article['content'])
        
        print(f"  ğŸ’¾ {filename}")
    
    print(f"âœ… æ•°æ®ä¿å­˜å®Œæˆï¼")
    return data_dir


def create_combined_corpus(data_dir: str) -> str:
    """åˆ›å»ºåˆå¹¶çš„è¯­æ–™åº“æ–‡ä»¶"""
    corpus_file = os.path.join(data_dir, "combined_corpus.txt")
    
    print("ğŸ“– åˆ›å»ºåˆå¹¶è¯­æ–™åº“...")
    
    with open(corpus_file, 'w', encoding='utf-8') as outfile:
        for filename in sorted(os.listdir(data_dir)):
            if filename.endswith('.txt') and filename != 'combined_corpus.txt':
                filepath = os.path.join(data_dir, filename)
                
                with open(filepath, 'r', encoding='utf-8') as infile:
                    content = infile.read()
                    outfile.write(content)
                    outfile.write("\n\n" + "="*80 + "\n\n")
    
    # è·å–æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(corpus_file)
    print(f"âœ… åˆå¹¶è¯­æ–™åº“åˆ›å»ºå®Œæˆ: {corpus_file}")
    print(f"   æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB")
    
    return corpus_file


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ çŸ¥è¯†å›¾è°±æµ‹è¯•æ•°æ®ä¸‹è½½å™¨")
    print("=" * 60)
    
    # ä¸‹è½½æ•°æ®
    all_articles = []
    
    # 1. æŠ€æœ¯æ–‡ç« 
    tech_articles = download_tech_articles()
    all_articles.extend(tech_articles)
    
    # 2. æ–°é—»æ–‡ç« 
    news_articles = download_news_articles()
    all_articles.extend(news_articles)
    
    # 3. ç»´åŸºç™¾ç§‘æ–‡ç« ï¼ˆå¯é€‰ï¼‰
    wiki_topics = [
        "äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "ç¥ç»ç½‘ç»œ",
        "Python", "JavaScript", "æ•°æ®åº“", "ç®—æ³•",
        "æ“ä½œç³»ç»Ÿ", "è®¡ç®—æœºç½‘ç»œ", "è½¯ä»¶å·¥ç¨‹", "äº‘è®¡ç®—"
    ]
    
    try:
        wiki_articles = download_wikipedia_articles(wiki_topics[:5])  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤šè¯·æ±‚
        all_articles.extend(wiki_articles)
    except Exception as e:
        print(f"âš ï¸ ç»´åŸºç™¾ç§‘ä¸‹è½½å¤±è´¥: {e}")
    
    print(f"\nğŸ“Š æ•°æ®ä¸‹è½½æ±‡æ€»:")
    print(f"   æ€»æ–‡ç« æ•°: {len(all_articles)}")
    
    source_counts = {}
    for article in all_articles:
        source = article["source"]
        source_counts[source] = source_counts.get(source, 0) + 1
    
    for source, count in source_counts.items():
        print(f"   {source}: {count} ç¯‡")
    
    # ä¿å­˜æ–‡ç« 
    data_dir = save_articles_to_files(all_articles)
    
    # åˆ›å»ºåˆå¹¶è¯­æ–™åº“
    corpus_file = create_combined_corpus(data_dir)
    
    print(f"\nğŸ¯ æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼")
    print(f"   æ•°æ®ç›®å½•: {data_dir}")
    print(f"   åˆå¹¶è¯­æ–™åº“: {corpus_file}")
    print(f"\nğŸ“ ä¸‹ä¸€æ­¥:")
    print(f"   è¿è¡Œå¤§è§„æ¨¡æµ‹è¯•: python scripts/test_large_scale_kg.py")
    
    return data_dir, corpus_file


if __name__ == "__main__":
    main()

