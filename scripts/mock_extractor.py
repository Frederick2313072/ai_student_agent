"""
æ¨¡æ‹ŸçŸ¥è¯†æŠ½å–å™¨

ç”¨äºæµ‹è¯•çš„æ¨¡æ‹ŸLLMæŠ½å–å™¨ï¼Œä¸ä¾èµ–çœŸå®APIå¯†é’¥ã€‚
"""

import re
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.feynman.core.graph.schema import KnowledgeTriple

logger = logging.getLogger(__name__)


class MockKnowledgeExtractor:
    """æ¨¡æ‹ŸçŸ¥è¯†æŠ½å–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨¡æ‹ŸæŠ½å–å™¨"""
        self.patterns = self._create_extraction_patterns()
    
    def _create_extraction_patterns(self):
        """åˆ›å»ºæŠ½å–æ¨¡å¼"""
        return [
            # å®šä¹‰æ¨¡å¼: Xæ˜¯Y
            (r'(\w+)æ˜¯([^ï¼Œã€‚]+)', "æ˜¯"),
            # åŒ…å«æ¨¡å¼: XåŒ…å«Y, XåŒ…æ‹¬Y
            (r'(\w+)åŒ…å«([^ï¼Œã€‚]+)', "åŒ…å«"),
            (r'(\w+)åŒ…æ‹¬([^ï¼Œã€‚]+)', "åŒ…å«"),
            # ç”¨äºæ¨¡å¼: Xç”¨äºY, Xåº”ç”¨äºY
            (r'(\w+)ç”¨äº([^ï¼Œã€‚]+)', "ç”¨äº"),
            (r'(\w+)åº”ç”¨äº([^ï¼Œã€‚]+)', "ç”¨äº"),
            # æ”¯æŒæ¨¡å¼: Xæ”¯æŒY
            (r'(\w+)æ”¯æŒ([^ï¼Œã€‚]+)', "æ”¯æŒ"),
            # åŸºäºæ¨¡å¼: XåŸºäºY
            (r'(\w+)åŸºäº([^ï¼Œã€‚]+)', "åŸºäº"),
            # æä¾›æ¨¡å¼: Xæä¾›Y
            (r'(\w+)æä¾›([^ï¼Œã€‚]+)', "æä¾›"),
            # ä½¿ç”¨æ¨¡å¼: Xä½¿ç”¨Y
            (r'(\w+)ä½¿ç”¨([^ï¼Œã€‚]+)', "ä½¿ç”¨"),
            # å¼€å‘æ¨¡å¼: Xå¼€å‘Y, Xåˆ›å»ºY
            (r'(\w+)å¼€å‘([^ï¼Œã€‚]+)', "å¼€å‘"),
            (r'(\w+)åˆ›å»º([^ï¼Œã€‚]+)', "å¼€å‘"),
            # å±äºæ¨¡å¼: Xå±äºY
            (r'(\w+)å±äº([^ï¼Œã€‚]+)', "å±äº"),
            # è¿æ¥æ¨¡å¼: Xè¿æ¥Y, Xå…³è”Y
            (r'(\w+)è¿æ¥([^ï¼Œã€‚]+)', "è¿æ¥"),
            (r'(\w+)å…³è”([^ï¼Œã€‚]+)', "ç›¸å…³"),
        ]
    
    async def extract_triples(self, text: str, source: Optional[str] = None) -> List[KnowledgeTriple]:
        """
        ä½¿ç”¨è§„åˆ™æ¨¡å¼æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„
        """
        triples = []
        
        # é¢„å¤„ç†æ–‡æœ¬
        text = self._preprocess_text(text)
        
        # åº”ç”¨æŠ½å–æ¨¡å¼
        for pattern, predicate in self.patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            
            for match in matches:
                subject = match.group(1).strip()
                object_entity = match.group(2).strip()
                
                # æ¸…ç†å’ŒéªŒè¯
                subject = self._clean_entity(subject)
                object_entity = self._clean_entity(object_entity)
                
                if self._is_valid_entity(subject) and self._is_valid_entity(object_entity):
                    triple = KnowledgeTriple(
                        subject=subject,
                        predicate=predicate,
                        object=object_entity,
                        confidence=0.75,  # æ¨¡æ‹Ÿç½®ä¿¡åº¦
                        source=source,
                        timestamp=datetime.now()
                    )
                    triples.append(triple)
        
        # é¢å¤–çš„å®ä½“-å…³ç³»æŠ½å–
        additional_triples = self._extract_additional_relations(text, source)
        triples.extend(additional_triples)
        
        # å»é‡
        unique_triples = self._deduplicate_triples(triples)
        
        logger.info(f"æ¨¡æ‹ŸæŠ½å–å™¨ä»æ–‡æœ¬æŠ½å–åˆ° {len(unique_triples)} ä¸ªä¸‰å…ƒç»„")
        return unique_triples
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\sï¼Œã€‚ï¼šï¼›ï¼ï¼Ÿ]', '', text)
        return text.strip()
    
    def _clean_entity(self, entity: str) -> str:
        """æ¸…ç†å®ä½“åç§°"""
        # ç§»é™¤å¸¸è§çš„æ— æ„ä¹‰è¯æ±‡
        stop_words = {'ä¸€ä¸ª', 'ä¸€ç§', 'è¿™ä¸ª', 'é‚£ä¸ª', 'æŸä¸ª', 'å¤šä¸ª', 'å¾ˆå¤š', 'è®¸å¤š', 'æ‰€æœ‰', 'å„ç§'}
        
        words = entity.split()
        cleaned_words = [word for word in words if word not in stop_words]
        
        return ' '.join(cleaned_words).strip()
    
    def _is_valid_entity(self, entity: str) -> bool:
        """éªŒè¯å®ä½“æ˜¯å¦æœ‰æ•ˆ"""
        if not entity or len(entity) < 2:
            return False
        
        # æ’é™¤æ— æ„ä¹‰çš„å®ä½“
        invalid_patterns = [
            r'^\d+$',  # çº¯æ•°å­—
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+$',  # çº¯ä¸­æ–‡æ•°å­—
            r'^[çš„äº†åœ¨å’Œä¸åŠå…¶æˆ–è€…]$',  # å¸¸è§è¿è¯
        ]
        
        for pattern in invalid_patterns:
            if re.match(pattern, entity):
                return False
        
        return True
    
    def _extract_additional_relations(self, text: str, source: Optional[str] = None) -> List[KnowledgeTriple]:
        """æŠ½å–é¢å¤–çš„å…³ç³»"""
        additional_triples = []
        
        # æ—¶é—´å…³ç³»æŠ½å–: åœ¨Xå¹´
        year_pattern = r'åœ¨?(\d{4})å¹´[^ï¼Œã€‚]*?([^ï¼Œã€‚]{2,10})([å‘å¸ƒ|æ¨å‡º|åˆ›å»º|æˆç«‹])'
        year_matches = re.finditer(year_pattern, text)
        
        for match in year_matches:
            year = match.group(1)
            entity = self._clean_entity(match.group(2))
            action = match.group(3)
            
            if self._is_valid_entity(entity):
                triple = KnowledgeTriple(
                    subject=entity,
                    predicate=f"{action}äº",
                    object=f"{year}å¹´",
                    confidence=0.8,
                    source=source,
                    timestamp=datetime.now()
                )
                additional_triples.append(triple)
        
        # åˆ†ç±»å…³ç³»æŠ½å–: XåŒ…æ‹¬Aã€Bã€C
        category_pattern = r'(\w+)[åŒ…æ‹¬|åŒ…å«][ï¼š:]([^ã€‚]+)'
        category_matches = re.finditer(category_pattern, text)
        
        for match in category_matches:
            category = self._clean_entity(match.group(1))
            items_text = match.group(2)
            
            # åˆ†å‰²é¡¹ç›®
            items = re.split(r'[ã€ï¼Œ,å’Œä¸åŠ]', items_text)
            
            for item in items:
                item = self._clean_entity(item.strip())
                
                if self._is_valid_entity(category) and self._is_valid_entity(item):
                    triple = KnowledgeTriple(
                        subject=item,
                        predicate="å±äº",
                        object=category,
                        confidence=0.7,
                        source=source,
                        timestamp=datetime.now()
                    )
                    additional_triples.append(triple)
        
        return additional_triples
    
    def _deduplicate_triples(self, triples: List[KnowledgeTriple]) -> List[KnowledgeTriple]:
        """å»é‡ä¸‰å…ƒç»„"""
        seen_ids = set()
        unique_triples = []
        
        for triple in triples:
            triple_id = triple.get_id()
            if triple_id not in seen_ids:
                seen_ids.add(triple_id)
                unique_triples.append(triple)
        
        return unique_triples
    
    async def extract_from_file(self, file_path: str) -> List[KnowledgeTriple]:
        """ä»æ–‡ä»¶ä¸­æŠ½å–çŸ¥è¯†ä¸‰å…ƒç»„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            # å¯¹äºå¤§æ–‡ä»¶ï¼Œåˆ†æ®µå¤„ç†
            if len(text) > 10000:  # 10KBä»¥ä¸Šçš„æ–‡ä»¶åˆ†æ®µå¤„ç†
                chunks = self._split_text(text)
                all_triples = []
                for i, chunk in enumerate(chunks):
                    chunk_source = f"{file_path}#chunk_{i}"
                    chunk_triples = await self.extract_triples(chunk, chunk_source)
                    all_triples.extend(chunk_triples)
                return all_triples
            else:
                return await self.extract_triples(text, file_path)
                
        except Exception as e:
            logger.error(f"ä»æ–‡ä»¶æŠ½å–å¤±è´¥ {file_path}: {e}")
            return []
    
    def _split_text(self, text: str, chunk_size: int = 2000, overlap: int = 200) -> List[str]:
        """æ–‡æœ¬åˆ†å—"""
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = min(start + chunk_size, text_length)
            
            # å°è¯•åœ¨å¥å·æˆ–æ¢è¡Œç¬¦å¤„åˆ†å‰²
            if end < text_length:
                split_pos = text.rfind('ã€‚', start, end)
                if split_pos == -1:
                    split_pos = text.rfind('\n', start, end)
                if split_pos != -1:
                    end = split_pos + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = max(end - overlap, start + 1)
        
        return chunks


# åˆ›å»ºæ¨¡æ‹ŸæœåŠ¡
async def create_mock_kg_service():
    """åˆ›å»ºä½¿ç”¨æ¨¡æ‹ŸæŠ½å–å™¨çš„çŸ¥è¯†å›¾è°±æœåŠ¡"""
    from src.feynman.core.graph.service import KnowledgeGraphService
    from src.feynman.core.graph.storage import NetworkXStorage
    from src.feynman.core.graph.builder import KnowledgeGraphBuilder
    
    # åˆ›å»ºæœåŠ¡å®ä¾‹
    kg_service = KnowledgeGraphService()
    
    # æ›¿æ¢ä¸ºæ¨¡æ‹ŸæŠ½å–å™¨
    kg_service.extractor = MockKnowledgeExtractor()
    
    return kg_service


if __name__ == "__main__":
    import asyncio
    
    async def test_mock_extractor():
        """æµ‹è¯•æ¨¡æ‹ŸæŠ½å–å™¨"""
        extractor = MockKnowledgeExtractor()
        
        test_text = """
        Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåœ¨1991å¹´å‘å¸ƒã€‚
        Pythonæ”¯æŒé¢å‘å¯¹è±¡ç¼–ç¨‹å’Œå‡½æ•°å¼ç¼–ç¨‹ã€‚
        æœºå™¨å­¦ä¹ åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚
        """
        
        triples = await extractor.extract_triples(test_text, "test")
        
        print("ğŸ§ª æ¨¡æ‹ŸæŠ½å–å™¨æµ‹è¯•:")
        print(f"è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(test_text)}")
        print(f"æŠ½å–ä¸‰å…ƒç»„æ•°: {len(triples)}")
        
        for i, triple in enumerate(triples, 1):
            print(f"{i}. {triple.subject} --[{triple.predicate}]--> {triple.object} (ç½®ä¿¡åº¦: {triple.confidence})")
    
    asyncio.run(test_mock_extractor())
