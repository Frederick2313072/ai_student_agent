
# 梯度下降 (Gradient Descent)

## 核心思想
梯度下降是一种广泛应用于机器学习和深度学习中的迭代优化算法，其核心目标是寻找函数（通常是损失函数或成本函数）的最小值。
想象一个人被困在浓雾弥漫的山上，他想尽快走到山谷的最低点。由于有浓雾，他看不清全局路径，但他可以感知脚下地面的坡度。一个最直观的策略是：观察当前位置哪个方向的坡度最大、下降最快，然后朝着那个方向走一小步。到达新位置后，他会再次重复这个过程，一步一步地，最终理论上能够到达山谷的最低点。

## 关键概念
1.  **损失函数 (Loss Function)**: 一个衡量模型预测值与真实值之间差异的函数。我们的目标就是最小化这个函数的值。
2.  **梯度 (Gradient)**: 在多维空间中，梯度是一个向量，指向函数值增长最快的方向。因此，梯度的反方向（负梯度）就是函数值下降最快的方向。
3.  **学习率 (Learning Rate, α)**: 这就是“下山”时每一步的步长。它是一个超参数，控制着每次参数更新的幅度。
    *   **学习率过大**: 可能会导致在最低点附近“振荡”或“跳过”最低点，无法收敛。
    *   **学习率过小**: 会导致收敛速度非常慢，需要大量的迭代次数才能到达最低点。

## 算法流程
1.  **初始化**: 随机选择一个初始参数点（w₀）。
2.  **计算梯度**: 在当前参数点计算损失函数的梯度。
3.  **更新参数**: 沿着负梯度方向更新参数，更新的幅度由学习率决定。公式为：w_new = w_old - α * ∇f(w_old)
4.  **迭代**: 重复步骤2和3，直到满足停止条件（如达到最大迭代次数，或梯度值接近于零）。

## 与其他概念的关系
*   **反向传播 (Backpropagation)**: 在深度学习中，反向传播是计算神经网络损失函数梯度的有效方法。梯度下降则利用反向传播计算出的梯度来更新网络的权重。可以说，反向传播为梯度下降提供了“方向盘”。 