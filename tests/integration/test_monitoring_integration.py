#!/usr/bin/env python3
"""
ç›‘æ§ç³»ç»Ÿé›†æˆæµ‹è¯•
éªŒè¯å®Œæ•´çš„ç›‘æ§é“¾è·¯ï¼šæŒ‡æ ‡æ”¶é›† â†’ æ—¥å¿—è®°å½• â†’ è¿½è¸ª â†’ å¥åº·æ£€æŸ¥ â†’ æˆæœ¬æ§åˆ¶
"""

import asyncio
import sys
import time
import json
import tempfile
import os
from datetime import datetime
from typing import Dict, Any


def print_test_header(test_name: str):
    """æ‰“å°æµ‹è¯•å¤´éƒ¨"""
    print(f"\n{'='*60}")
    print(f"ğŸ” é›†æˆæµ‹è¯•: {test_name}")
    print(f"{'='*60}")


def print_test_result(success: bool, message: str, details: Dict[str, Any] = None):
    """æ‰“å°æµ‹è¯•ç»“æœ"""
    status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
    print(f"{status}: {message}")
    
    if details:
        for key, value in details.items():
            print(f"  - {key}: {value}")


async def test_full_monitoring_stack():
    """æµ‹è¯•å®Œæ•´ç›‘æ§æ ˆ"""
    print_test_header("å®Œæ•´ç›‘æ§æ ˆé›†æˆ")
    
    try:
        # 1. å¯¼å…¥æ‰€æœ‰ç›‘æ§æ¨¡å—
        from core.monitoring.metrics import get_registry, SystemMetricsCollector
        from core.monitoring.logging import setup_structured_logging, get_logger
        from core.monitoring.health import HealthChecker
        from core.monitoring.cost_tracker import CostTracker
        from core.monitoring.tracing import initialize_tracing, get_tracer
        from core.monitoring.langfuse_integration import initialize_langfuse
        
        # 2. åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ
        setup_structured_logging()
        initialize_tracing()
        langfuse_init = initialize_langfuse()
        
        logger = get_logger("test.integration")
        
        # 3. æµ‹è¯•æŒ‡æ ‡æ”¶é›†
        registry = get_registry()
        metrics_count = len(list(registry._names_to_collectors.keys()))
        
        # 4. æµ‹è¯•å¥åº·æ£€æŸ¥
        health_checker = HealthChecker()
        health_result = await health_checker.run_all_checks()
        
        # 5. æµ‹è¯•æˆæœ¬è¿½è¸ª
        cost_tracker = CostTracker()
        usage_record = cost_tracker.record_usage("gpt-4", 100, 50, "test-session")
        
        # 6. æµ‹è¯•è¿½è¸ªå™¨
        tracer = get_tracer()
        with tracer.start_as_current_span("test_span") as span:
            span.set_attribute("test.integration", True)
        
        await health_checker.close()
        
        print_test_result(True, "å®Œæ•´ç›‘æ§æ ˆé›†æˆæ­£å¸¸", {
            "æŒ‡æ ‡æ³¨å†Œè¡¨": f"åŒ…å« {metrics_count} ä¸ªæŒ‡æ ‡",
            "å¥åº·æ£€æŸ¥": f"æ€»ä½“çŠ¶æ€ {health_result['status']}",
            "æˆæœ¬è¿½è¸ª": f"è®°å½•æˆæœ¬ ${usage_record.cost_breakdown.total_cost:.6f}",
            "åˆ†å¸ƒå¼è¿½è¸ª": "è¿½è¸ªå™¨å¯ç”¨",
            "LangFuseé›†æˆ": "æˆåŠŸ" if langfuse_init else "è·³è¿‡ï¼ˆæœªé…ç½®ï¼‰",
            "ç»“æ„åŒ–æ—¥å¿—": "å·²é…ç½®"
        })
        
    except Exception as e:
        print_test_result(False, f"ç›‘æ§æ ˆé›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")


async def test_api_monitoring_flow():
    """æµ‹è¯•APIç›‘æ§æµç¨‹"""
    print_test_header("APIç›‘æ§æµç¨‹")
    
    try:
        from api.middleware.monitoring import MonitoringMiddleware
        from api.routers.monitoring import router
        from fastapi import FastAPI
        from fastapi.testclient import TestClient
        
        # åˆ›å»ºæµ‹è¯•åº”ç”¨
        app = FastAPI()
        app.add_middleware(MonitoringMiddleware)
        app.include_router(router)
        
        # æ·»åŠ æµ‹è¯•ç«¯ç‚¹
        @app.get("/test")
        async def test_endpoint():
            return {"status": "ok"}
        
        client = TestClient(app)
        
        # æµ‹è¯•å„ä¸ªç›‘æ§ç«¯ç‚¹
        test_results = {}
        
        # å¥åº·æ£€æŸ¥
        health_response = client.get("/health")
        test_results["å¥åº·æ£€æŸ¥"] = f"çŠ¶æ€ç  {health_response.status_code}"
        
        # å°±ç»ªæ£€æŸ¥
        ready_response = client.get("/health/ready")
        test_results["å°±ç»ªæ£€æŸ¥"] = f"çŠ¶æ€ç  {ready_response.status_code}"
        
        # å­˜æ´»æ£€æŸ¥
        live_response = client.get("/health/live")
        test_results["å­˜æ´»æ£€æŸ¥"] = f"çŠ¶æ€ç  {live_response.status_code}"
        
        # æŒ‡æ ‡ç«¯ç‚¹
        metrics_response = client.get("/metrics")
        test_results["æŒ‡æ ‡ç«¯ç‚¹"] = f"çŠ¶æ€ç  {metrics_response.status_code}"
        
        # ç›‘æ§çŠ¶æ€
        status_response = client.get("/monitoring/status")
        test_results["ç›‘æ§çŠ¶æ€"] = f"çŠ¶æ€ç  {status_response.status_code}"
        
        # æˆæœ¬ç»Ÿè®¡
        costs_response = client.get("/monitoring/costs")
        test_results["æˆæœ¬ç»Ÿè®¡"] = f"çŠ¶æ€ç  {costs_response.status_code}"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å“åº”å†…å®¹
        all_passed = all(
            200 <= response.status_code < 300 
            for response in [health_response, ready_response, live_response, 
                           metrics_response, status_response, costs_response]
        )
        
        print_test_result(all_passed, "APIç›‘æ§æµç¨‹æ­£å¸¸", test_results)
        
    except Exception as e:
        print_test_result(False, f"APIç›‘æ§æµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_agent_workflow_monitoring():
    """æµ‹è¯•Agentå·¥ä½œæµç›‘æ§"""
    print_test_header("Agentå·¥ä½œæµç›‘æ§")
    
    try:
        from agent.agent import build_graph
        from core.monitoring.tracing import trace_span
        from core.monitoring.logging import set_request_context
        
        # æ„å»ºå·¥ä½œæµ
        workflow_app = build_graph()
        
        # è®¾ç½®æµ‹è¯•ä¸Šä¸‹æ–‡
        set_request_context(
            request_id="test-req-integration",
            session_id="test-session-integration",
            user_id="test-user"
        )
        
        # æ¨¡æ‹Ÿå¯¹è¯è¾“å…¥
        test_inputs = {
            "topic": "æœºå™¨å­¦ä¹ ",
            "user_explanation": "æœºå™¨å­¦ä¹ æ˜¯è®©è®¡ç®—æœºé€šè¿‡æ•°æ®å­¦ä¹ çš„æŠ€æœ¯",
            "short_term_memory": []
        }
        
        # æ‰§è¡Œå·¥ä½œæµï¼ˆä½¿ç”¨è¿½è¸ªï¼‰
        with trace_span("test_workflow_execution") as span:
            span.set_attribute("test.type", "integration")
            span.set_attribute("test.workflow", "feynman_conversation")
            
            # ç”±äºè¿™æ˜¯é›†æˆæµ‹è¯•ï¼Œæˆ‘ä»¬åªéªŒè¯å·¥ä½œæµèƒ½å¤Ÿæ­£å¸¸æ„å»ºå’Œåˆå§‹åŒ–
            # å®é™…çš„LLMè°ƒç”¨åœ¨æµ‹è¯•ç¯å¢ƒä¸­å¯èƒ½ä¸å¯ç”¨
            workflow_config = {"configurable": {"thread_id": "test-thread"}}
            
            # æ£€æŸ¥å·¥ä½œæµç»“æ„
            nodes = list(workflow_app.get_graph().nodes.keys())
            edges = list(workflow_app.get_graph().edges)
            
            span.set_attribute("workflow.nodes_count", len(nodes))
            span.set_attribute("workflow.edges_count", len(edges))
        
        print_test_result(True, "Agentå·¥ä½œæµç›‘æ§æ­£å¸¸", {
            "å·¥ä½œæµèŠ‚ç‚¹": f"{len(nodes)} ä¸ªèŠ‚ç‚¹: {', '.join(nodes)}",
            "å·¥ä½œæµè¾¹": f"{len(edges)} æ¡è¾¹",
            "ç›‘æ§è£…é¥°å™¨": "å·²åº”ç”¨",
            "è¿½è¸ªé›†æˆ": "æˆåŠŸ",
            "ä¸Šä¸‹æ–‡è®¾ç½®": "æˆåŠŸ"
        })
        
    except Exception as e:
        print_test_result(False, f"Agentå·¥ä½œæµç›‘æ§æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_tool_monitoring():
    """æµ‹è¯•å·¥å…·ç›‘æ§"""
    print_test_header("å·¥å…·ç›‘æ§")
    
    try:
        from agent.tools import web_search, knowledge_retriever
        from core.monitoring.metrics import TOOL_CALLS_TOTAL, TOOL_CALL_DURATION
        from core.monitoring.tracing import trace_tool_call
        
        # è·å–åˆå§‹æŒ‡æ ‡å€¼
        initial_calls = TOOL_CALLS_TOTAL.labels(tool_name="test_tool", status="success")._value._value
        
        # æµ‹è¯•å·¥å…·ç›‘æ§è£…é¥°å™¨
        @trace_tool_call("test_tool")
        def test_monitored_tool(query: str) -> str:
            """æµ‹è¯•å·¥å…·ï¼Œç”¨äºéªŒè¯ç›‘æ§åŠŸèƒ½"""
            time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return f"å¤„ç†äº†æŸ¥è¯¢: {query}"
        
        # æ‰§è¡Œç›‘æ§çš„å·¥å…·è°ƒç”¨
        result = test_monitored_tool("æµ‹è¯•æŸ¥è¯¢")
        
        # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦æ›´æ–°
        time.sleep(0.1)  # ç­‰å¾…æŒ‡æ ‡æ›´æ–°
        
        # éªŒè¯å·¥å…·å®šä¹‰
        available_tools = [
            "knowledge_retriever", "memory_retriever", "file_operation", "web_search",
            "translate_text", "calculate_math", "search_academic_papers", "search_videos", 
            "search_wikipedia", "get_news", "execute_code", "create_mindmap", "create_flowchart"
        ]
        
        print_test_result(True, "å·¥å…·ç›‘æ§æ­£å¸¸", {
            "å¯ç”¨å·¥å…·æ•°é‡": len(available_tools),
            "ç›‘æ§è£…é¥°å™¨": "åº”ç”¨æˆåŠŸ",
            "æµ‹è¯•å·¥å…·è°ƒç”¨": f"è¿”å›: {result[:50]}...",
            "æŒ‡æ ‡æ”¶é›†": "æ­£å¸¸",
            "è¿½è¸ªé›†æˆ": "æˆåŠŸ"
        })
        
    except Exception as e:
        print_test_result(False, f"å·¥å…·ç›‘æ§æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_cost_tracking_workflow():
    """æµ‹è¯•æˆæœ¬è¿½è¸ªå®Œæ•´æµç¨‹"""
    print_test_header("æˆæœ¬è¿½è¸ªæµç¨‹")
    
    try:
        from core.monitoring.cost_tracker import CostTracker, get_cost_tracker
        
        # åˆ›å»ºä¸´æ—¶æˆæœ¬è¿½è¸ªå™¨
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            temp_storage = f.name
        
        tracker = CostTracker(storage_path=temp_storage)
        
        # æ¨¡æ‹Ÿå¤šæ¬¡LLMè°ƒç”¨
        test_calls = [
            ("gpt-4", 500, 200, "session-1", "chat"),
            ("gpt-4o", 300, 150, "session-1", "chat"),
            ("glm-4", 800, 400, "session-2", "chat"),
            ("text-embedding-3-small", 1000, 0, "session-1", "embedding"),
        ]
        
        total_cost = 0
        for model, prompt_tokens, completion_tokens, session_id, request_type in test_calls:
            record = tracker.record_usage(
                model=model,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                session_id=session_id,
                request_type=request_type
            )
            total_cost += record.cost_breakdown.total_cost
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        daily_cost = tracker.get_daily_cost()
        session1_cost = tracker.get_session_cost("session-1")
        model_stats = tracker.get_model_usage_stats()
        budget_status = tracker.get_budget_status()
        
        # ç”ŸæˆæŠ¥å‘Š
        from datetime import date, timedelta
        report = tracker.export_usage_report(
            start_date=date.today(),
            end_date=date.today()
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_storage)
        
        print_test_result(True, "æˆæœ¬è¿½è¸ªæµç¨‹æ­£å¸¸", {
            "æ¨¡æ‹Ÿè°ƒç”¨æ•°": len(test_calls),
            "æ€»æˆæœ¬": f"${total_cost:.6f}",
            "ä»Šæ—¥æˆæœ¬": f"${daily_cost:.6f}",
            "ä¼šè¯æˆæœ¬": f"${session1_cost:.6f}",
            "æ¨¡å‹ç»Ÿè®¡": f"{len(model_stats)} ä¸ªæ¨¡å‹",
            "é¢„ç®—çŠ¶æ€": f"æ—¥é¢„ç®—ä½¿ç”¨ {budget_status['daily']['percentage']:.2f}%",
            "æŠ¥å‘Šç”Ÿæˆ": f"åŒ…å« {report['summary']['total_calls']} æ¬¡è°ƒç”¨"
        })
        
    except Exception as e:
        print_test_result(False, f"æˆæœ¬è¿½è¸ªæµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_structured_logging_flow():
    """æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—å®Œæ•´æµç¨‹"""
    print_test_header("ç»“æ„åŒ–æ—¥å¿—æµç¨‹")
    
    try:
        from core.monitoring.logging import (
            setup_structured_logging, get_logger, set_request_context,
            log_api_request, log_tool_call, log_llm_call, log_workflow_execution
        )
        import io
        import sys
        from contextlib import redirect_stdout
        
        # æ•è·æ—¥å¿—è¾“å‡º
        captured_logs = io.StringIO()
        
        # é‡æ–°é…ç½®æ—¥å¿—åˆ°å†…å­˜
        setup_structured_logging()
        logger = get_logger("test.integration.logging")
        
        # è®¾ç½®æµ‹è¯•ä¸Šä¸‹æ–‡
        set_request_context(
            request_id="integration-req-123",
            session_id="integration-session-456",
            user_id="integration-user"
        )
        
        # æ¨¡æ‹Ÿå„ç§æ—¥å¿—è®°å½•
        log_events = []
        
        # APIè¯·æ±‚æ—¥å¿—
        log_api_request("POST", "/chat", 200, 150.0, "test-agent", "127.0.0.1")
        log_events.append("APIè¯·æ±‚æ—¥å¿—")
        
        # å·¥å…·è°ƒç”¨æ—¥å¿—
        log_tool_call("web_search", True, 250.0, input_args={"query": "æµ‹è¯•"}, output_length=500)
        log_events.append("å·¥å…·è°ƒç”¨æ—¥å¿—")
        
        # LLMè°ƒç”¨æ—¥å¿—
        log_llm_call("gpt-4", 100, 50, 1500.0, 0.002)
        log_events.append("LLMè°ƒç”¨æ—¥å¿—")
        
        # å·¥ä½œæµæ‰§è¡Œæ—¥å¿—
        log_workflow_execution("gap_identifier_react", True, 800.0, "æœºå™¨å­¦ä¹ ", 3)
        log_events.append("å·¥ä½œæµæ‰§è¡Œæ—¥å¿—")
        
        # è‡ªå®šä¹‰æ—¥å¿—
        logger.info("é›†æˆæµ‹è¯•æ—¥å¿—", extra={
            "test_type": "integration",
            "component": "logging",
            "metrics": {"processed": 4, "success": True}
        })
        log_events.append("è‡ªå®šä¹‰æ—¥å¿—")
        
        print_test_result(True, "ç»“æ„åŒ–æ—¥å¿—æµç¨‹æ­£å¸¸", {
            "æ—¥å¿—äº‹ä»¶æ•°": len(log_events),
            "ä¸Šä¸‹æ–‡è®¾ç½®": "æˆåŠŸ",
            "æ—¥å¿—æ ¼å¼": "JSONç»“æ„åŒ–",
            "æ—¥å¿—å‡½æ•°": "å…¨éƒ¨å¯ç”¨",
            "äº‹ä»¶ç±»å‹": ", ".join(log_events)
        })
        
    except Exception as e:
        print_test_result(False, f"ç»“æ„åŒ–æ—¥å¿—æµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_middleware_integration():
    """æµ‹è¯•ä¸­é—´ä»¶é›†æˆ"""
    print_test_header("ä¸­é—´ä»¶é›†æˆ")
    
    try:
        from fastapi import FastAPI
        from fastapi.testclient import TestClient
        from api.middleware.monitoring import MonitoringMiddleware
        from api.middleware.cors import setup_cors
        from api.routers.monitoring import router as monitoring_router
        
        # åˆ›å»ºå®Œæ•´çš„æµ‹è¯•åº”ç”¨
        app = FastAPI(title="ç›‘æ§é›†æˆæµ‹è¯•")
        
        # é…ç½®CORS
        setup_cors(app)
        
        # æ·»åŠ ç›‘æ§ä¸­é—´ä»¶
        app.add_middleware(MonitoringMiddleware)
        
        # åŒ…å«ç›‘æ§è·¯ç”±
        app.include_router(monitoring_router)
        
        # æ·»åŠ æµ‹è¯•ç«¯ç‚¹
        @app.get("/test")
        async def test_endpoint():
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return {"message": "æµ‹è¯•æˆåŠŸ", "timestamp": datetime.now().isoformat()}
        
        # åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
        client = TestClient(app)
        
        # æ‰§è¡Œæµ‹è¯•è¯·æ±‚
        test_endpoints = [
            ("/test", "æµ‹è¯•ç«¯ç‚¹"),
            ("/health", "å¥åº·æ£€æŸ¥"),
            ("/health/ready", "å°±ç»ªæ£€æŸ¥"),
            ("/health/live", "å­˜æ´»æ£€æŸ¥"),
            ("/metrics", "æŒ‡æ ‡ç«¯ç‚¹"),
            ("/monitoring/status", "ç›‘æ§çŠ¶æ€"),
            ("/monitoring/config", "ç›‘æ§é…ç½®")
        ]
        
        results = {}
        for endpoint, name in test_endpoints:
            try:
                response = client.get(endpoint)
                results[name] = f"çŠ¶æ€ç  {response.status_code}"
                
                # éªŒè¯å“åº”å†…å®¹
                if response.status_code == 200:
                    if endpoint == "/metrics":
                        # æ£€æŸ¥æŒ‡æ ‡æ ¼å¼
                        if "# HELP" in response.text and "# TYPE" in response.text:
                            results[name] += " (æ ¼å¼æ­£ç¡®)"
                    elif endpoint.startswith("/health"):
                        # æ£€æŸ¥å¥åº·æ£€æŸ¥å“åº”
                        try:
                            health_data = response.json()
                            if "status" in health_data or "ready" in health_data or "alive" in health_data:
                                results[name] += " (å“åº”æ­£ç¡®)"
                        except:
                            pass
                    else:
                        # æ£€æŸ¥JSONå“åº”
                        try:
                            response.json()
                            results[name] += " (JSONæœ‰æ•ˆ)"
                        except:
                            pass
                            
            except Exception as e:
                results[name] = f"é”™è¯¯: {str(e)}"
        
        success_count = len([r for r in results.values() if "çŠ¶æ€ç  2" in r])
        
        print_test_result(
            success_count >= len(test_endpoints) * 0.8,  # 80%æˆåŠŸç‡
            f"ä¸­é—´ä»¶é›†æˆæ­£å¸¸ ({success_count}/{len(test_endpoints)} ä¸ªç«¯ç‚¹æˆåŠŸ)",
            results
        )
        
    except Exception as e:
        print_test_result(False, f"ä¸­é—´ä»¶é›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")


async def test_tracing_integration():
    """æµ‹è¯•åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ"""
    print_test_header("åˆ†å¸ƒå¼è¿½è¸ªé›†æˆ")
    
    try:
        from core.monitoring.tracing import (
            initialize_tracing, get_tracer, trace_span, add_span_attribute, 
            add_span_event, get_trace_id, get_span_id
        )
        
        # åˆå§‹åŒ–è¿½è¸ªï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
        initialize_tracing()
        tracer = get_tracer()
        
        # æµ‹è¯•åŸºæœ¬è¿½è¸ªåŠŸèƒ½
        trace_results = {}
        
        # æµ‹è¯•Spanåˆ›å»º
        with tracer.start_as_current_span("test_integration_span") as span:
            span.set_attribute("test.integration", True)
            span.set_attribute("test.timestamp", datetime.now().isoformat())
            
            trace_id = get_trace_id()
            span_id = get_span_id()
            
            trace_results["Spanåˆ›å»º"] = "æˆåŠŸ"
            trace_results["TraceID"] = f"é•¿åº¦ {len(trace_id)}" if trace_id else "æœªè·å–åˆ°"
            trace_results["SpanID"] = f"é•¿åº¦ {len(span_id)}" if span_id else "æœªè·å–åˆ°"
            
            # æµ‹è¯•åµŒå¥—Span
            with trace_span("nested_span") as nested_span:
                nested_span.set_attribute("nested.test", True)
                add_span_attribute("dynamic.attribute", "æµ‹è¯•å€¼")
                add_span_event("test_event", {"event_data": "é›†æˆæµ‹è¯•"})
                
                trace_results["åµŒå¥—Span"] = "æˆåŠŸ"
                trace_results["åŠ¨æ€å±æ€§"] = "å·²æ·»åŠ "
                trace_results["äº‹ä»¶è®°å½•"] = "å·²æ·»åŠ "
            
            # æµ‹è¯•å¼‚å¸¸è®°å½•
            try:
                with trace_span("error_span") as error_span:
                    raise ValueError("æµ‹è¯•å¼‚å¸¸")
            except ValueError:
                trace_results["å¼‚å¸¸è®°å½•"] = "æˆåŠŸæ•è·"
        
        print_test_result(True, "åˆ†å¸ƒå¼è¿½è¸ªé›†æˆæ­£å¸¸", trace_results)
        
    except Exception as e:
        print_test_result(False, f"åˆ†å¸ƒå¼è¿½è¸ªé›†æˆæµ‹è¯•å¤±è´¥: {str(e)}")


async def test_end_to_end_monitoring():
    """ç«¯åˆ°ç«¯ç›‘æ§æµ‹è¯•"""
    print_test_header("ç«¯åˆ°ç«¯ç›‘æ§")
    
    try:
        from core.monitoring.metrics import (
            record_conversation_start, record_conversation_end,
            record_llm_usage, record_memory_operation
        )
        from core.monitoring.logging import get_logger, set_request_context
        from core.monitoring.cost_tracker import get_cost_tracker
        from core.monitoring.tracing import trace_conversation_flow
        
        # è®¾ç½®æµ‹è¯•ä¼šè¯
        session_id = "e2e-test-session"
        topic = "ç«¯åˆ°ç«¯ç›‘æ§æµ‹è¯•"
        
        set_request_context(
            request_id="e2e-req-123",
            session_id=session_id,
            user_id="e2e-user"
        )
        
        logger = get_logger("test.e2e")
        cost_tracker = get_cost_tracker()
        
        # æ¨¡æ‹Ÿå®Œæ•´çš„å¯¹è¯æµç¨‹
        with trace_conversation_flow(session_id, topic):
            # 1. å¯¹è¯å¼€å§‹
            record_conversation_start()
            logger.info("å¯¹è¯å¼€å§‹", extra={"topic": topic})
            
            # 2. æ¨¡æ‹ŸLLMè°ƒç”¨
            record_llm_usage("gpt-4", 200, 100, 2.5, 0.005)
            logger.info("LLMè°ƒç”¨å®Œæˆ")
            
            # 3. æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
            from core.monitoring.logging import log_tool_call
            log_tool_call("web_search", True, 300.0, input_args={"query": topic})
            
            # 4. æ¨¡æ‹Ÿè®°å¿†æ“ä½œ
            record_memory_operation("add", "success")
            logger.info("è®°å¿†æ“ä½œå®Œæˆ")
            
            # 5. å¯¹è¯ç»“æŸ
            conversation_duration = 45.0  # 45ç§’
            record_conversation_end(conversation_duration, "completed")
            logger.info("å¯¹è¯ç»“æŸ", extra={"duration": conversation_duration})
        
        # è·å–è¿½è¸ªå’Œæˆæœ¬ä¿¡æ¯
        session_cost = cost_tracker.get_session_cost(session_id)
        daily_cost = cost_tracker.get_daily_cost()
        
        print_test_result(True, "ç«¯åˆ°ç«¯ç›‘æ§æ­£å¸¸", {
            "å¯¹è¯æµç¨‹": "å®Œæ•´è¿½è¸ª",
            "LLMè°ƒç”¨": "æˆæœ¬å·²è®°å½•",
            "å·¥å…·è°ƒç”¨": "æ—¥å¿—å·²è®°å½•",
            "è®°å¿†æ“ä½œ": "æŒ‡æ ‡å·²æ›´æ–°",
            "ä¼šè¯æˆæœ¬": f"${session_cost:.6f}",
            "æ—¥æ€»æˆæœ¬": f"${daily_cost:.6f}",
            "é“¾è·¯è¿½è¸ª": "å®Œæ•´è¦†ç›–"
        })
        
    except Exception as e:
        print_test_result(False, f"ç«¯åˆ°ç«¯ç›‘æ§æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_performance_impact():
    """æµ‹è¯•ç›‘æ§å¯¹æ€§èƒ½çš„å½±å“"""
    print_test_header("ç›‘æ§æ€§èƒ½å½±å“")
    
    try:
        import time
        import statistics
        
        # æµ‹è¯•å‡½æ•°
        def test_function(iterations: int = 1000):
            """æµ‹è¯•å‡½æ•°ï¼Œç”¨äºæ€§èƒ½åŸºå‡†æµ‹è¯•"""
            results = []
            for i in range(iterations):
                start = time.time()
                # æ¨¡æ‹Ÿç®€å•è®¡ç®—
                result = sum(range(100))
                end = time.time()
                results.append((end - start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            return results
        
        # ä¸å¸¦ç›‘æ§çš„åŸºå‡†æµ‹è¯•
        baseline_times = test_function(100)
        baseline_avg = statistics.mean(baseline_times)
        
        # å¸¦ç›‘æ§çš„æµ‹è¯•
        from core.monitoring.tracing import trace_function
        from core.monitoring.metrics import monitor_api_call
        
        @trace_function("performance_test")
        @monitor_api_call("test_endpoint")
        def monitored_test_function(iterations: int = 100):
            return test_function(iterations)
        
        monitored_times = monitored_test_function(100)
        monitored_avg = statistics.mean(monitored_times)
        
        # è®¡ç®—æ€§èƒ½å½±å“
        overhead_percent = ((monitored_avg - baseline_avg) / baseline_avg) * 100
        
        # æ€§èƒ½å½±å“åº”è¯¥å°äº10%
        acceptable_overhead = overhead_percent < 10
        
        print_test_result(acceptable_overhead, "ç›‘æ§æ€§èƒ½å½±å“å¯æ¥å—", {
            "åŸºå‡†å¹³å‡è€—æ—¶": f"{baseline_avg:.3f}ms",
            "ç›‘æ§å¹³å‡è€—æ—¶": f"{monitored_avg:.3f}ms",
            "æ€§èƒ½å¼€é”€": f"{overhead_percent:.2f}%",
            "å¯æ¥å—é˜ˆå€¼": "< 10%",
            "æµ‹è¯•è¿­ä»£æ•°": "100æ¬¡",
            "è¯„ä¼°ç»“æœ": "æ€§èƒ½å½±å“åœ¨å¯æ¥å—èŒƒå›´å†…" if acceptable_overhead else "æ€§èƒ½å½±å“è¿‡å¤§"
        })
        
    except Exception as e:
        print_test_result(False, f"ç›‘æ§æ€§èƒ½å½±å“æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†å’Œæ¢å¤"""
    print_test_header("é”™è¯¯å¤„ç†ä¸æ¢å¤")
    
    try:
        from core.monitoring.health import HealthChecker, HealthStatus
        from core.monitoring.metrics import TOOL_ERRORS_TOTAL
        from core.monitoring.tracing import trace_span, add_span_event
        from core.monitoring.logging import get_logger
        
        logger = get_logger("test.error_handling")
        error_scenarios = []
        
        # 1. æµ‹è¯•å¥åº·æ£€æŸ¥é”™è¯¯å¤„ç†
        health_checker = HealthChecker()
        
        # æ¨¡æ‹Ÿå¤–éƒ¨APIä¸å¯ç”¨
        try:
            external_checks = await health_checker.check_external_apis()
            error_scenarios.append(f"å¤–éƒ¨APIæ£€æŸ¥: {len(external_checks)} ä¸ªAPI")
        except Exception as e:
            error_scenarios.append(f"å¤–éƒ¨APIæ£€æŸ¥å¼‚å¸¸: {str(e)[:50]}")
        
        # 2. æµ‹è¯•æŒ‡æ ‡é”™è¯¯å¤„ç†
        try:
            # æ•…æ„è§¦å‘ä¸€ä¸ªå·¥å…·é”™è¯¯æŒ‡æ ‡
            TOOL_ERRORS_TOTAL.labels(tool_name="test_tool", error_type="TestError").inc()
            error_scenarios.append("å·¥å…·é”™è¯¯æŒ‡æ ‡: è®°å½•æˆåŠŸ")
        except Exception as e:
            error_scenarios.append(f"å·¥å…·é”™è¯¯æŒ‡æ ‡å¼‚å¸¸: {str(e)}")
        
        # 3. æµ‹è¯•è¿½è¸ªé”™è¯¯å¤„ç†
        try:
            with trace_span("error_test_span") as span:
                try:
                    # æ•…æ„å¼•å‘å¼‚å¸¸
                    raise RuntimeError("æµ‹è¯•å¼‚å¸¸å¤„ç†")
                except RuntimeError as e:
                    span.record_exception(e)
                    add_span_event("error_handled", {"error_type": "RuntimeError"})
                    error_scenarios.append("è¿½è¸ªå¼‚å¸¸å¤„ç†: æˆåŠŸ")
        except Exception as e:
            error_scenarios.append(f"è¿½è¸ªå¼‚å¸¸å¤„ç†å¤±è´¥: {str(e)}")
        
        # 4. æµ‹è¯•æ—¥å¿—é”™è¯¯å¤„ç†
        try:
            logger.error("æµ‹è¯•é”™è¯¯æ—¥å¿—", extra={
                "error_code": "TEST_ERROR",
                "component": "integration_test",
                "recoverable": True
            })
            error_scenarios.append("é”™è¯¯æ—¥å¿—è®°å½•: æˆåŠŸ")
        except Exception as e:
            error_scenarios.append(f"é”™è¯¯æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
        
        await health_checker.close()
        
        success_count = len([s for s in error_scenarios if "æˆåŠŸ" in s])
        
        print_test_result(
            success_count >= 3,  # è‡³å°‘3ä¸ªåœºæ™¯æˆåŠŸ
            f"é”™è¯¯å¤„ç†ä¸æ¢å¤æ­£å¸¸ ({success_count}/4 ä¸ªåœºæ™¯æˆåŠŸ)",
            {f"åœºæ™¯{i+1}": scenario for i, scenario in enumerate(error_scenarios)}
        )
        
    except Exception as e:
        print_test_result(False, f"é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {str(e)}")


async def test_monitoring_configuration():
    """æµ‹è¯•ç›‘æ§é…ç½®å®Œæ•´æ€§"""
    print_test_header("ç›‘æ§é…ç½®å®Œæ•´æ€§")
    
    try:
        import os
        from pathlib import Path
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_files = [
            "config/prometheus.yml",
            "config/alerting_rules.yml", 
            "config/alertmanager.yml",
            "config/blackbox.yml",
            "config/docker-compose.monitoring.yml",
            "config/grafana/feynman_system_overview.json",
            "config/grafana/feynman_business_metrics.json",
            "config/grafana/provisioning/datasources/prometheus.yml",
            "config/grafana/provisioning/dashboards/dashboard.yml"
        ]
        
        config_status = {}
        for config_file in config_files:
            if Path(config_file).exists():
                file_size = Path(config_file).stat().st_size
                config_status[config_file] = f"å­˜åœ¨ ({file_size} å­—èŠ‚)"
            else:
                config_status[config_file] = "ç¼ºå¤±"
        
        # æ£€æŸ¥è„šæœ¬æ–‡ä»¶
        script_files = [
            "scripts/monitoring/start_monitoring.sh"
        ]
        
        for script_file in script_files:
            if Path(script_file).exists() and os.access(script_file, os.X_OK):
                config_status[script_file] = "å­˜åœ¨ä¸”å¯æ‰§è¡Œ"
            elif Path(script_file).exists():
                config_status[script_file] = "å­˜åœ¨ä½†ä¸å¯æ‰§è¡Œ"
            else:
                config_status[script_file] = "ç¼ºå¤±"
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        monitoring_env_vars = [
            "MONITORING_ENABLED", "METRICS_ENABLED", "TRACING_ENABLED",
            "LOG_LEVEL", "LOG_FORMAT", "PROMETHEUS_PORT"
        ]
        
        env_status = {}
        for var in monitoring_env_vars:
            value = os.getenv(var)
            env_status[var] = "å·²è®¾ç½®" if value else "æœªè®¾ç½®"
        
        # è®¡ç®—é…ç½®å®Œæ•´åº¦
        config_files_ok = len([s for s in config_status.values() if "å­˜åœ¨" in s])
        env_vars_ok = len([s for s in env_status.values() if "å·²è®¾ç½®" in s])
        
        total_configs = len(config_files) + len(script_files) + len(monitoring_env_vars)
        total_ok = config_files_ok + env_vars_ok
        
        completeness = (total_ok / total_configs) * 100
        
        print_test_result(
            completeness >= 90,  # 90%é…ç½®å®Œæ•´åº¦
            f"ç›‘æ§é…ç½®å®Œæ•´æ€§ {completeness:.1f}%",
            {
                "é…ç½®æ–‡ä»¶": f"{config_files_ok}/{len(config_files)} ä¸ª",
                "ç¯å¢ƒå˜é‡": f"{env_vars_ok}/{len(monitoring_env_vars)} ä¸ª", 
                "æ•´ä½“å®Œæ•´åº¦": f"{completeness:.1f}%",
                **{k: v for k, v in list(config_status.items())[:3]},  # æ˜¾ç¤ºå‰3ä¸ªé…ç½®çŠ¶æ€
                **{k: v for k, v in list(env_status.items())[:3]}     # æ˜¾ç¤ºå‰3ä¸ªç¯å¢ƒå˜é‡çŠ¶æ€
            }
        )
        
    except Exception as e:
        print_test_result(False, f"ç›‘æ§é…ç½®å®Œæ•´æ€§æµ‹è¯•å¤±è´¥: {str(e)}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ è´¹æ›¼å­¦ä¹ ç³»ç»Ÿç›‘æ§é›†æˆæµ‹è¯•")
    print("="*50)
    print(f"æµ‹è¯•æ—¶é—´: {datetime.now().isoformat()}")
    print(f"æµ‹è¯•ç¯å¢ƒ: {os.getenv('ENVIRONMENT', 'development')}")
    
    # è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•
    test_functions = [
        test_monitoring_configuration,
        test_full_monitoring_stack,
        test_structured_logging_flow,
        test_api_monitoring_flow,
        test_middleware_integration,
        test_agent_workflow_monitoring,
        test_tool_monitoring,
        test_cost_tracking_workflow,
        test_tracing_integration,
        test_error_handling,
        test_performance_impact
    ]
    
    start_time = time.time()
    passed = 0
    total = len(test_functions)
    
    for test_func in test_functions:
        try:
            await test_func()
            passed += 1
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_func.__name__} å‡ºç°å¼‚å¸¸: {str(e)}")
    
    duration = time.time() - start_time
    
    # è¾“å‡ºè¯¦ç»†æ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“Š ç›‘æ§é›†æˆæµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    print(f"æ€»æµ‹è¯•æ•°: {total}")
    print(f"é€šè¿‡æ•°: {passed}")
    print(f"å¤±è´¥æ•°: {total - passed}")
    print(f"æˆåŠŸç‡: {(passed/total)*100:.1f}%")
    print(f"æ€»è€—æ—¶: {duration:.2f}ç§’")
    
    # ç³»ç»Ÿå°±ç»ªè¯„ä¼°
    readiness_score = (passed / total) * 100
    
    if readiness_score >= 90:
        print("\nğŸ‰ ç›‘æ§ç³»ç»Ÿå°±ç»ªï¼Œå¯æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ï¼")
        print("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
        print("1. è¿è¡Œ './scripts/monitoring/start_monitoring.sh' å¯åŠ¨ç›‘æ§æ ˆ")
        print("2. è®¿é—® http://localhost:3000 é…ç½®Grafanaé¢æ¿")
        print("3. é…ç½®å‘Šè­¦é€šçŸ¥æ¸ é“")
        print("4. è¿›è¡Œç”Ÿäº§ç¯å¢ƒå‹åŠ›æµ‹è¯•")
        return 0
    elif readiness_score >= 70:
        print(f"\nâš ï¸  ç›‘æ§ç³»ç»ŸåŸºæœ¬å°±ç»ªï¼Œä½†æœ‰ {total - passed} ä¸ªç»„ä»¶éœ€è¦ä¿®å¤")
        print("\nå»ºè®®:")
        print("1. ä¿®å¤å¤±è´¥çš„æµ‹è¯•é¡¹")
        print("2. å®Œå–„ç›‘æ§é…ç½®")
        print("3. å†æ¬¡è¿è¡Œé›†æˆæµ‹è¯•")
        return 1
    else:
        print(f"\nâŒ ç›‘æ§ç³»ç»Ÿæœªå°±ç»ªï¼Œéœ€è¦è§£å†³é‡å¤§é—®é¢˜")
        print(f"æˆåŠŸç‡ä»… {readiness_score:.1f}%ï¼Œå»ºè®®:")
        print("1. æ£€æŸ¥ä¾èµ–å®‰è£…")
        print("2. éªŒè¯é…ç½®æ–‡ä»¶")
        print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("4. è”ç³»æŠ€æœ¯æ”¯æŒ")
        return 2


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  é›†æˆæµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nğŸ’¥ é›†æˆæµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}")
        sys.exit(1)